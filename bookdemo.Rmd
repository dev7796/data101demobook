--- 
title: "DATA 101 Shortest Textbook"
subtitle: "How To Accomplish More With Less"
author: 
    name: "Tomasz Imielinski"
    email: "timielinski@gmail.com"
# subauthor: 
#     name: "Deep Lokhande"
#     email: "deep.lokhande@rutgers.edu"
# date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
# bibliography: [book.bib, packages.bib]
# biblio-style: apalike
# link-citations: yes
description: "This is a example of a interactive book for DATA 101 Course thought by Prof. Tomasz Imielinski (http://data101.cs.rutgers.edu/) Find the demo site at (https://deeplokhande.github.io/data101demobook/) Published using Bookdown for R."
---



# {-}

---

<!-- <script src=https://cdn.datacamp.com/datacamp-light-latest.min.js></script> -->
<script src="files/js/dcl.js"></script>


```{r ,include=FALSE}
tutorial::go_interactive( greedy = FALSE)

```

<!-- <span style="color: red;background-color: lightgrey;font-size: 25px;">NOTE: THE DATACAMP SERVER ARE CURRENTLY DOWN. PLEASE COPY PASTE THE CODE IN RSTUDIO TO RUN OFFLINE. WE'LL TRY TO GET THE SITE WORKING AGAIN SHORTLY.</span> -->
<!-- <!-- **$\color{red}{\text{{NOTE: THE DATACAMP SERVER ARE CURRENTLY DOWN. PLEASE COPY PASTE THE CODE IN RSTUDIO TO RUN OFFLINE. WE'LL TRY TO GET THE SITE WORKING AGAIN SHORTLY.}}}$** -->


This is a textbook based on the [DATA 101](http://data101.cs.rutgers.edu/) course thought by Prof. Tomasz Imielinski at Computer Science Department at Rutgers University, New Brunswick.

Data 101 is an introductory course for beginners from any field of study, interested in the field of Data Science. 

<!-- _Acknowledgment_: We thank the significant contributions of **[Deep Lokhande](deeplokhande.github.io)** towards building this book. -->

This book's pages are created using [Rmarkdown](https://rmarkdown.rstudio.com/) from Rstudio (similar to jupyter notebook) and the book is compiled using ["bookdown"](https://cran.r-project.org/web/packages/bookdown/index.html) package.

The most important aspect of this interactive book are the interactive code chunks for running code, which are powered by a minimal version of Datacamp's learning interface called `Datacamp Light`. For more info visit [Datacamp Light](https://github.com/datacamp/datacamp-light)

- Note1: This book is undergoing constant updates following along with the course thought in Spring 2021. Topics under progress are marked with a " * ".

- Note2: This book uses Datacamp Light for supporting runnable code chunks. In case the code chunks do not connect to run-time session, please copy the code and run in RStudio. Also, please report if facing this issue to the instructor via [email](mailto:timielinski@gmail.com).

---





<!-- **FIRST FEW PAGES CAN BE SKIPPED. JUMP TO RECITAION 3 FOR DEMO** -->

<!-- This is a _sample_ book written in **Markdown**. You can use anything that Pandoc's Markdown supports, e.g., a math equation $a^2 + b^2 = c^2$. -->

<!-- The **bookdown** package can be installed from CRAN or Github: -->

<!-- ```{r eval=FALSE} -->
<!-- install.packages("bookdown") -->
<!-- # or the development version -->
<!-- # devtools::install_github("rstudio/bookdown") -->
<!-- ``` -->

<!-- Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading `#`. -->

<!-- To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): <https://yihui.org/tinytex/>. -->

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# Introduction {#intro}

<script src="files/js/dcl.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()

```


<!-- You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods). -->

<!-- Figures and tables with captions will be placed in `figure` and `table` environments, respectively. -->


<!-- ```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'} -->
<!-- par(mar = c(4, 4, .1, .1)) -->
<!-- plot(pressure, type = 'b', pch = 19) -->
<!-- ``` -->

<!-- Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab). -->

<!-- ```{r nice-tab, tidy=FALSE} -->
<!-- knitr::kable( -->
<!--   head(iris, 20), caption = 'Here is a nice table!', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- ``` -->

<!-- You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015]. -->



The objective of this textbook is to provide you the shortest path to exploring your data, visualizing it, forming hypotheses and validating and defending them. Given a data set, you want to be able to make any plot you wish, find plots which show something actionable and interesting, explore data by slicing and dicing it and finally present your results in statistically convincing manner, perhaps in colorful and visually appealing way.

Questions which you will have to anticipate and you will have to answer are
- How do you know that your findings are not random?
- And fundamental of all questions:
- **So what?**

Even the most impressing looking results may come up randomly. And you will be asked this question along with the question *“what was your p-value and how did you compute it”*

And even if you convince your audience that your results are not random, you will have to be ready to explain why should you audience care about the results you reported. In other words, is there any actionable value in your results? Or they are just simply interesting, good to know, but no one really needs to care much about them otherwise? Hopefully it is the former not the latter. 


In the following sections we will address these questions and go through the process of data exploration, validation, and presentation.

- We will start with making plots, follow with free style data exploration – which allows us to form the leads, that is hypotheses. Then we will follow with simple statistical tests which will allow  us to validate these hypothesis and defend our findings against randomness claims. - We will learn how to calculate p-values and how to use them to defend our findings. 
- We will use as few R commands as possible and reach our goal in shortest possible path.  In fact we will demonstrate how using just 7 R commands we can perform  quite sophisticated data exploration.

## Setting Up R 

- **Important Instructions**
  - Installation of R is required before installing RStudio
    - "R” is a programming language, and,
    - “RStudio” is an Integrated Development Environment (IDE) which provides you a platform to code in R.

- __How to download and install R & RStudio?__

  - _Downloading and installing R._

    - For Windows Users.
      - Click on the link provided below or copy paste it on your favourite browser and go to the website.
          - [https://cran.r-project.org/bin/windows/base/](https://cran.r-project.org/bin/windows/base/)
      - Click on the link at top left where it says “Download R 4.0.3 for windows” or the latest at the time of your installation.
      - Open the downloaded file and follow the instructions as it is.

    - For MAC Users.
      - Click on the link provided below or copy paste it on your favourite browser and go to the website.
          - [https://cloud.r-project.org/bin/macosx/](https://cloud.r-project.org/bin/macosx/)
      - Under “Latest release”, click on “R-4.0.3.pkg” or the latest at the time of your installation.
      - Open the downloaded file and follow the instructions as it is.
      
 
  - _Downloading and installing RStudio._
  
    - For Windows Users.
      - Click on the link below or copy paste it in your favourite browser.
          - [https://rstudio.com/products/rstudio/download/](https://rstudio.com/products/rstudio/download/)
      - Scroll down almost till the end of the web page until you find a section named “All Installers”.
      - Click on the download link beside “Windows 10/8/7” to download the windows version of RStudio.
      - Install RStudio by clicking on the downloaded file and following the instructions as it is.

    - For MAC Users.
      - Click on the link below or copy paste it in your favourite browser.
          - [https://rstudio.com/products/rstudio/download/](https://rstudio.com/products/rstudio/download/)
      - Scroll down almost till the end of the web page until you find a section named “All Installers”.
      - Click on the link beside “macOS 10.13+” to start your download the MAC version of RStudio.
      - Install RStudio by clicking on the downloaded file and following the instructions as it is.

---

- __How to upload a data set?__

 - To upload the dataset/file present in csv format the read.csv() and read.csv2() functions are frequently used The read.csv() and read.csv2() have different separator symbol: for the former this is a comma, whereas the latter uses a semicolon.
 - Let us look at the example.
 
```{r, tut=TRUE}
# Read in the data
df <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv")

# Print out `df`
head(df)
```

  

<!--chapter:end:chapters/intro.Rmd-->

# Data Exploration {#dataexp}

<script src="files/js/dcl.js"></script>
```{r ,include=FALSE}
tutorial::go_interactive( greedy = FALSE)

```


## Plots {#plots}


```{r,echo=FALSE}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load
# head(moody)

temp<-knitr::kable(
  head(moody, 10), caption = 'Snippet of Moody Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

<!-- --- -->

<!-- ### Topics visited in this sub-chapter -->

<!-- * Scatter Plot -->
<!-- * Barplot -->
<!-- * Boxplot -->
<!-- * Mosaic Plot -->

---

### Scatter Plot

- Scatter Plot are used to plot points on the Cartesian plane (X-Y Plane)
- Hence it is used when both the labels are numerical values.


Lets look at example of scatter plot using Moody.
```{r,tut=TRUE,height=700}
# Let's look at a 2 attribute scatter plot.
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load
plot(moody$participation,moody$score,ylab="score",xlab="participation",main=" Participation vs Score",col="red")


```


---

### Bar Plot

- A bar plot is used to plot rectangular bars proportional to the values present in a numerical vector. 
- This rectangle height is proportional to the value of the variable in the vector.
- Barplots are also used to graphically represent the distribution of a categorical variable, after converting the categorical vector into a table(i.e. frequency distribution table)
- In a bar plot, you can also give different colors to each bar.



```{r, tut=TRUE,height=700}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

#lets make a table for the grades of students and counts of students for each Grade. 

t<-table(moody$grade)

#once we have the table lets create a barplot for it.

barplot(t,xlab="Grade",ylab="Number of Students",col=colors,
main="Barplot for student grade distribution",border="black")
```


---

###  Box Plot

- A boxplot shows the distribution of data in a dataset. 
- A boxplot shows the following things:
  - Minimum
  - Maximum
  - Median
  - First quartile
  - Third quartile
  - Outliers
- You can create a single boxplot using just a vector or a multiple boxplot using a formula.
- When you write a formula, you should use the Tilde (~) operator. This column name on the left side of this operator goes on the y axis and the column name on the right side of this operator goes on the x axis.



```{r,tut=TRUE,height=700}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars


#Suppose you want to find the distribution of students score per Grade. We use box plot for getting that. 
boxplot(score~grade,data=moody,xlab="Grade",ylab="Score", main="Boxplot of grade vs score",col=colors,border="black")

# the circles represent outliers.
```


<!-- ## 4. Histogram -->

<!-- Refer Slide 15. -->

<!-- ```{r} -->

<!-- #Suppose you want to find the frequecy/distribution of cars with mileage in particular range. We use histogram for this.  -->

<!-- hist(automobile$`city-mpg`,xlim = c(0,100),xlab = 'milage', main = "Histogram of Car milage",col=colors,border="black") -->

<!-- # You can Change column range using breaks. -->

<!-- ``` -->


<!-- For more detail,reference and example refer Slides -->

---
###  Mosiac Plot

- Mosaic plot is a graphical method for visualizing data from two or more qualitative variables.
- The length of the rectangles in the mosaic plot represents the frequency of that particular value.
- The width and length of the mosaic plot can be used to interpret the frequencies of the elements.
-For example, if you want to plot the number of individuals per letter grade using a smartphone, you want to look at mosiac plot.


```{r,tut=TRUE,height=700}
# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load
colors<- c('red','blue','cyan','yellow','green') # Assigning different colors to bars

#suppose you want to find numbers of students with a particular grade based on their texting habits. Use Mosiac-plot.

mosaicplot(moody$grade~moody$texting,xlab = 'Grade',ylab = 'Texting habit', main = "Mosiac of grade vs texing habit in class",col=colors,border="black")


```


```{r child="./chapters/freestyle.Rmd"}

```

<!-- ```{r child="./chapters/datatransformation.Rmd"} -->

<!-- ``` -->


<!--chapter:end:chapters/plots.Rmd-->

# Simple Statistical Evaluation {#stateval}

<script src="files/js/dcl.js"></script>
```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE,height=700)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

The biggest enemy of your findings is randomness. In order to convince your audience that you have found something you need to address the question “how do you know your result is simply sheer luck, it is random?”

This is where you need statistical tests for use in hypothesis testing. 

---

#### Two Important Formula's:

- _Mean_
\begin{equation}
\bar{X}=\frac{\sum{X}}{N}
\ \text{where, X is set of numbers and 
N is size of set.}
\end{equation}

- _Standard Deviation_

\begin{equation}

\sigma = \sqrt{\frac{\sum{(X - \mu)^2}}{N}}\\
\text{where, X is set of numbers, $\mu$ is average of set of numbers, }\\ \text{ N is size of the set, $\sigma$ is standard deviation}


\end{equation}

---

## Z-test {#ztest} 

A z-test is any statistical test used in hypothesis testing with an underlying normal distribution.

In other words, when the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution, z-test can be used.

Outcome of the z-test is the z-score which is a numerical measure to test the mean of a distribution.
z-score is measured in terms of standard deviation from mean.

### Steps for hypothesis testing using Z-test.

Running a Z-test requires 5 steps:

1. State the null hypothesis and the alternate hypothesis
    - Select a null hypothesis and an alternate hypothesis which will be tested using the z-test.
2. Choose an Alpha $\alpha$ level.
    - Usually this is selected to be small, such that the area under the normal distribution curve is accumulated most in the range between the alpha level.
    - Thus mostly in statistical testing, $\alpha = 0.05$ is selected.
3. Calculate the z-test statistic.
    - The z-test statistic is calculated using the z-score formula.
      \begin{equation} z = \frac{x-\mu}{\sigma}\text{ where, $z$ = z-score, $x$ = raw score, $\mu$ = mean and $\sigma$ = standard deviation } \end{equation}
4. Calculate the p-value using the z-score
    - Once we have the z-score we want to calculate the p-value from it.
    - To do this, there are 2 ways,
      - First use the z-table available online at [z-table.com](http://www.z-table.com/)
      - Second, use the pnorm() function in R to find the p-value.
5. Compare the p-value with $\alpha$
    - After getting the p-value from step 4, compare it with the $\alpha$ level we selected in step 2.
    - This decides if we can reject the null hypothesis or not.
      - If the p-value obtained is lower than $\alpha$, then we can reject the null hypothesis.
      - If the p-value is more than $\alpha$, we fail to reject the null hypothesis due to lack of significant evidence.
      
      
Some important relation between one-sided and two sided test while using hypothesis testing is as follows:

- First, estimate the expected value $\mu$ of T(statistic) under the null hypothesis, and obtain an estimate $\sigma$ of the standard deviation of T.
- Second, determine the properties of T : one tailed or two tailed.
  - For Null hypothesis H0: $\mu \geq \mu_0$ vs alternative hypothesis H1: $\mu < \mu_0$ , it is upper/right-tailed (one tailed).
  - For Null hypothesis H0:$\mu \leq \mu_0$ vs alternative hypothesis H1: $\mu > \mu_0$ , it is lower/left-tailed (one tailed).
  - For Null hypothesis H0: $\mu = \mu_0$ vs alternative hypothesis H1: $\mu \neq \mu_0$ , it is two-tailed.
- Once you calculate the pnorm() in step 4, depending on the properties of two as described above, 
  - use `pnorm(-Z)` for right tailed tests,
  - use `2*pnorm(-Z)` for two tailed test, and 
  - use `pnorm(Z)` for left tailed tests.
  - *Note*: (Here Z = z-score). Also the method mentioned above works similar to that studied in class/recitations, but is simple to understand, and does not require subtracting the pnorm() output from 1.

---

### Z-test Example 1 (Right Sided)
Now lets look at an example to use this z-test for hypothesis testing.

We will study the example to statistically find the relation of the traffic volume per minute between two tunnels, namely Holland and Lincoln .

```{r,echo=FALSE,error=FALSE,warning=FALSE}
library(kableExtra)
earningdata<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/TRAFFIC.csv") #web load
temp<-knitr::kable(earningdata[sample(nrow(earningdata),10),], caption = 'Snippet of Traffic Dataset',booktabs=TRUE) 
kableExtra::scroll_box(temp,width = "100%")
```

Thus stating out Null Hypothesis and Alternate Hypothesis.

- Null Hypothesis H0: Traffic in Lincoln is same as Traffic in Holland tunnel.
- Alternate Hypothesis H1: Traffic in Lincoln is higher than traffic in Holland tunnel.

Once we have stated our hypothesis, lets see the z-test in practice.

```{r}
# Load Dataset
TRAFFIC<-read.csv('https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/TRAFFIC.csv')
summary(TRAFFIC) #gives us the data statistics

#data clean and subset
lincoln.data <- subset(TRAFFIC, TRAFFIC$TUNNEL == "Lincoln")
holland.data <- subset(TRAFFIC, TRAFFIC$TUNNEL == "Holland")

# traffic at lincoln
# This variable is a column of 1401 rows.
lincoln.traffic <- lincoln.data$VOLUME_PER_MINUTE

# traffic at holland
# This variable is a column of 1401 rows.
holland.traffic <- holland.data$VOLUME_PER_MINUTE

# standard deviation of two samples.
# The final value is the standard deviation, in Volume per minute.
sd.lincoln <- sd(lincoln.traffic)
sd.holland <- sd(holland.traffic)

# means of two samples
mean.lincoln <- mean(lincoln.traffic)
mean.holland <- mean(holland.traffic)

# length of lincoln and holland
len_lincoln <- length(lincoln.traffic)
len_holland <- length(holland.traffic)

# standard deviation of traffic
sd.lin.hol <- sqrt(sd.lincoln^2/len_lincoln + sd.holland^2/len_holland)

# z score
zeta <- (mean.lincoln - mean.holland)/sd.lin.hol
zeta

# get p
p = pnorm(-zeta)
p

# plot the zeta value on the normal distribution curve.
plot(x=seq(from = -25, to= 25, by=0.1),y=dnorm(seq(from = -25, to= 25,  by=0.1),mean=0),type='l',xlab = 'mean difference',  ylab='possibility')
abline(v=zeta, col='red')
```

We can see that form the P-Value obtained is near to 0, which is less than 0.05. 

Hence, we reject the NULL Hypothesis and conclude with high degree of certainty that traffic in Lincoln is higher than traffic Holland. 


---

### Z-test Example 2 (Left Sided)
Now lets look at another example to use this z-test for hypothesis testing.

We will study the example to statistically find the relation between capital gains of people with two Zodiac Signs , namely Aquarius and Libra.

```{r,echo=FALSE,error=FALSE,warning=FALSE}
library(kableExtra)
earningdata<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/ZodiacChallenge.csv") #web load
temp<-knitr::kable(earningdata[sample(nrow(earningdata),10),], caption = 'Snippet of Zodiac Dataset',booktabs=TRUE) 
kableExtra::scroll_box(temp,width = "100%")
```

Now stating out Null Hypothesis and Alternate Hypothesis.

- Null Hypothesis H0: Capital Gains of people with Aquarius is same as people with Libra zodiac sign.
- Alternate Hypothesis H1: Capital Gains of people with Aquarius is lower than as people with Libra zodiac sign.

Once we have stated our hypothesis, lets see the z-test in practice.

```{r}
# Load Dataset
ZodiacData<-read.csv('https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/ZodiacChallenge.csv')
summary(ZodiacData) #gives us the data statistics

#data clean and subset
Aquarius.data <- subset(ZodiacData, ZodiacData$ZODIAK == "Aquarius")
Libra.data <- subset(ZodiacData, ZodiacData$ZODIAK == "Libra")

# Zodiac Aquarius
Aquarius.Zodiac <- Aquarius.data$CAPITALGAINS

# Zodiac  Libra
Libra.Zodiac <- Libra.data$CAPITALGAINS

# standard deviation of two samples.
sd.Aquarius <- sd(Aquarius.Zodiac)
sd.Libra <- sd(Libra.Zodiac)

# means of two samples
mean.Aquarius <- mean(Aquarius.Zodiac)
mean.Libra <- mean(Libra.Zodiac)

# length of Aquarius and Libra
len_Aquarius <- length(Aquarius.Zodiac)
len_Libra <- length(Libra.Zodiac)

# standard deviation
sd.aqu.lib <- sqrt(sd.Aquarius^2/len_Aquarius + sd.Libra^2/len_Libra)

# z score
zeta <- (mean.Aquarius - mean.Libra)/sd.aqu.lib
zeta

# get p
p = pnorm(zeta)
p

# plot the zeta value on the normal distribution curve.
plot(x=seq(from = -25, to= 25, by=0.1),y=dnorm(seq(from = -25, to= 25,  by=0.1),mean=0),type='l',xlab = 'mean difference',  ylab='possibility')
abline(v=zeta, col='red')
```
We can see that form the P-Value obtained is less than 0.05. 

Hence, we reject the NULL Hypothesis and conclude with high degree of certainty that Capital Gains of people with Aquarius is lower than as people with Libra zodiac sign.

---

### Z-test Example 3 (Two Tailed)

We will study the example to statistically find the relation between capital gains of people with two Countries, namely US and Columbia.

Now stating out Null Hypothesis and Alternate Hypothesis.

- Null Hypothesis H0: Capital Gains of people of United States is same as people of Colombia.
- Alternate Hypothesis H1: Capital Gains of people of United States is not equal to that of the people of Colombia.

Once we have stated our hypothesis, lets see the z-test in practice.

```{r}
# Load Dataset
ZodiacData<-read.csv('https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/ZodiacChallenge.csv')
summary(ZodiacData) #gives us the data statistics

#data clean and subset
US.data <- subset(ZodiacData, ZodiacData$NATIVE == "United-States")
Columbia.data <- subset(ZodiacData, ZodiacData$NATIVE == "Columbia")

# Country US
US.country <- US.data$CAPITALGAINS

# Country  Columbia
Columbia.country <- Columbia.data$CAPITALGAINS

# standard deviation of two samples.
sd.US <- sd(US.country)
sd.Columbia <- sd(Columbia.country)

# means of two samples
mean.US <- mean(US.country)
mean.Columbia <- mean(Columbia.country)

# length of US and Columbia
len_US <- length(US.country)
len_Columbia <- length(Columbia.country)

# standard deviation
sd.us.col <- sqrt(sd.US^2/len_US + sd.Columbia^2/len_Columbia)

# z score
zeta <- (mean.US - mean.Columbia)/sd.us.col
zeta

# get p
p = 2*pnorm(-zeta)
p

# plot the zeta value on the normal distribution curve.
plot(x=seq(from = -25, to= 25, by=0.1),y=dnorm(seq(from = -25, to= 25,  by=0.1),mean=0),type='l',xlab = 'mean difference',  ylab='possibility')
abline(v=zeta, col='red')
```

We can see that form the P-Value obtained is less than 0.05. 

Hence, we reject the NULL Hypothesis and conclude with high degree of certainty that Capital Gains of people of United States is not equal to that of the people of Colombia.

---

```{r child="./chapters/permutationtest.Rmd"}

```

---

## Multiple Hypothesis - Bonferroni Correction. {#bonferroni}

```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE,height=700)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

<script src="files/js/dcl.js"></script>

While dealing with the dataset with several number of dimensions, it is possible to get a lot of amazing and interesting insights and conclusions from it.

But, unfortunately, sometimes a lot of the data included in case of such large dataset, might be junk.

We can make multiple assumptions from such data. But, while doing so, we may consider some useless data/patterns that might hamper our results and lead to the pitfall of believing in hypotheses, that are not actually true.

This is common when performing multiple hypothesis testing.

Multiple hypothesis testing refers to any instance that involves the simultaneous testing of more than one hypothesis.

Let’s consider the example of Traffic dataset.

- We have given two tunnels ”Holland” and “Lincoln”, but what if we were given all the tunnels in the US? 
- We can make a lot of hypotheses in that case. 
- And for each set of hypothesis, would you still consider the value of α as 0.05 as the cut-off for P-value? 

It may seem to be a good idea to just go and check the p-value for any set of hypotheses with the cut-off value of $\alpha$ as 0.05. 

But this might not give you the correct answer always. 

If you have 100 different hypotheses to consider in the data, then the probability of getting at least one significant result with $\alpha = 0.05$ will be,
$$P(\text{at least one significant result}) = 1- (1-0.05)^{100} ≈ 0.99$$

This means that if we consider 0.05 as our cut-off value, then the probability of getting at least one significant result will be about 99%, which leads to overfitting of data and it clearly doesn’t give us proper idea about our hypothesis. 

Methods for dealing with multiple testing frequently call for adjusting $\alpha$ in some way, so that the probability of observing at least one significant result due to chance remains below your desired significance level.

One such method for adjusting $\alpha$ is *BONFERRONI CORRECTION!*

The Bonferroni correction sets the significance cut-off at $\alpha / N$ where N is the number of possible hypotheses. 

For example, in the example above, with 100 tests and $\alpha = 0.05$, you’d only reject a null hypothesis if the p-value is less than $\alpha/N = 0.05/100 = 0.0005$

Thus, the value of $\alpha$ after Bonferroni correction would be $0.0005$.

Again, let’s calculate the probability of observing at least one significant result when using the correction just described:

$$P(\text{at least one significant result}) = 1 − P(\text{no significant results}) \\
= 1 − (1 − 0.0005)^{100} ≈ 0.048$$

This gives us 4.8% probability of getting at least one significant result. 

As we can see this value of probability using Bonferroni correction is much better than the 99% which we saw before when  we did not use correction for performing multiple hypothesis testing.

But there are some downfall of using Bonferroni correction too. (Although for the scope of this course Bonferroni Correction works fine.)

  - The Bonferroni correction tends to be a bit too conservative. 
  - Also, we benefit here from assuming that all tests are independent of each other. In practical applications, that is often not the case. 
  - Depending on the correlation structure of the tests, the Bonferroni correction could beextremely conservative, leading to a high rate of false negatives.

---

### Examples for Multiple hypothesis testing.

Let’s consider the Happiness dataset as an example.
<script src="files/js/dcl.js"></script>


```{r,echo=FALSE,error=FALSE,warning=FALSE,tut=FALSE}
library(kableExtra)
earningdata<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/HAPPINESS2017.csv") #web load
temp<-knitr::kable(earningdata[sample(nrow(earningdata),10),], caption = 'Snippet of Happiness Dataset',booktabs=TRUE) 
kableExtra::scroll_box(temp,width = "100%")
```

There are 156 unique countries in the dataset. 
This can be checked using the unique() function – `unique(indiv_happiness$country)`

Since there are 156 distinct countries, we have ${{n}\choose{2}} = {156\choose2}=(156 * 155)/2 = 12090$ different hypotheses. Let’s call this value N. 

Using this N, the P-value cutoff after Bonferroni correction will be, $α = 0.05 / 12090 ≈ 4.13 *10^{-6}$

#### Example 1 

Let’s calculate the P-value for the following hypotheses from the dataset.

- Our hypothesis: People from Canada are happier than people from Iceland.
- Null hypothesis: There is no difference in happiness levels of people from Canada and people from Iceland.

<script src="files/js/dcl.js"></script>

```{r,tut=TRUE}
# Load dataset
happiness <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/HAPPINESS2017.csv", stringsAsFactors = T) #web load

# Two subsets of Canada and Iceland 
happiness.canada <- subset(happiness$HAPPINESS, happiness$COUNTRY =="Canada")
happiness.iceland <- subset(happiness$HAPPINESS, happiness$COUNTRY == "Iceland")

# Mean of subsets.
mean.canada <- mean(happiness.canada)
mean.iceland <- mean(happiness.iceland)

mean.canada
mean.iceland

# Length of subsets
len.canada <- length(happiness.canada)
len.iceland <- length(happiness.iceland)

# Standard Deviation of Subsets.
sd.canada <- sd(happiness.canada)
sd.iceland <- sd(happiness.iceland)

# Calculating Z-score 
zeta <- (mean.canada - mean.iceland)/ sqrt((sd.canada^2)/len.canada + (sd.iceland^2)/len.iceland)
zeta

# Calculate p-value from Z-score
p_value <- pnorm(-zeta)
p_value
```

In this case, after applying Bonferroni Correction we get the value of $α = 0.05/12090 ≈ 4.14 * 10^{-06}$
Here, we get the p-value of 0.25 which is much higher than the value of our α.
Based on this we fail reject our null hypothesis.

#### Example 2
<script src="files/js/dcl.js"></script>

Let’s consider the following hypotheses from the dataset.

- Our hypothesis: People from Italy are happier than people from Afghanistan.
- Null hypothesis: There is no difference in happiness levels of people from Italy and people from Afghanistan.

```{r,tut=TRUE}
# Load dataset
happiness <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/HAPPINESS2017.csv", stringsAsFactors = T) #web load

# Two subsets of Italy and Afghanistan 
happiness.italy <- subset(happiness$HAPPINESS, happiness$COUNTRY =="Italy")
happiness.afghanistan <- subset(happiness$HAPPINESS, happiness$COUNTRY == "Afghanistan")

# Mean of subsets.
mean.italy <- mean(happiness.italy)
mean.afghanistan <- mean(happiness.afghanistan)

mean.italy
mean.afghanistan

# Length of subsets
len.italy <- length(happiness.italy)
len.afghanistan <- length(happiness.afghanistan)

# Standard Deviation of Subsets.
sd.italy <- sd(happiness.italy)
sd.afghanistan <- sd(happiness.afghanistan)

# Calculating Z-score 
zeta <- (mean.italy - mean.afghanistan)/ sqrt((sd.italy^2)/len.italy + (sd.afghanistan^2)/len.afghanistan)
zeta

# Calculate p-value from Z-score
p_value <- pnorm(-zeta)
p_value
```
In this case, after applying Bonferroni Correction we get the value of $α = 0.05/12090 ≈ 4.14 * 10^{-06}$

Here, we get the p-value of 0.00364 which is lower than the value of default p-value cutoff $α = 0.05$, but this obtained p-value is higher than our Bonferroni correction cutoff.

So, based on the results, we fail to reject our null hypothesis even though the obtained p-value is less than 0.05.

---

**EOC**

<!--chapter:end:chapters/stateval.Rmd-->

# Revision of R commands {#revision}

<script src="files/js/dcl.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive()
```

In this chapter we are going to recap at some **basic** and useful functions we have used in R.
The examples we use here will be helpful in revising few of the functions we studied and would give an baseline of function we would need in the future while programming in R.

List of commands:

1. c()
2. data.frame()
3. subset()
4. table()
5. tapply()
6. cut()
7. summary(), mean(),length(), max(),min(), sd(),nrow(), ncol()
8. class()

And also all the plot commands present in section \@ref(plots)

---

## c() & data.frame() & class() {#dataframe}

- c() 
  - The c() function is used for combining arguments.
  - The default behavior of the c() method is to combine its arguments to form a vector. 
  - All arguments are coerced (forcibly converted) to a common type which is the type of the returned value.
  - For example,the non-character values are coerced to character type if one of the elements is a character.
  - the hierarchy followed is NULL < raw < logical < integer < double < complex < character < list < expression. 
  
- dataframe()
  - A data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column.
  - Following are the characteristics of a data frame.
    - The column names should be non-empty.
    - The row names should be unique.
    - The data stored in a data frame can be of numeric, factor or character type.
    - Each column should contain same number of data items.
    
- class()
  - The class() function has multiple uses, but for here, it is used to check the type of object.  
  
  
```{r,error=TRUE}

#Lets create 3 vectors with title, author and year.
title <- c('Data Smart','Orientalism','False Impressions','Making Software')
author <- c('Foreman, John','Said, Edward','Archer, Jeffery','Oram, Andy')
year <- c(2010,2011,2012,1998)

#Lets look at how the created vectors look.
title
author
year

# Also lets look at their types using the class function.
class(title)
class(author)
class(year)


# Now lets create a dataframe using the above column vectors.

df <- data.frame(title, author, year)
df # Lets look at how the dataframe looks.

```

---

## summary(), mean(),length(), max(),min(), sd(),nrow(), ncol(), dim() {#basicfunction}

The functions in this section are very simple yet are always useful to get more information from data.

- summary() function computes summary statistics of data.
- mean() function is used to find the average of the data.
- sd() fucntion is used to find the standard deviation of the data.
- length() function is used to get or set the length of data.
- max() function is used to get the maximum valued element in the data.
- min() function is used to get the minimum valued element in the data.
- nrow() function is used to find the number/count of the rows present in data.
- ncol() function is used to find the number/count of the columns present in data.
- dim() function is used to find the dimensions of the data.

Lets look at example of all these functions.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load dataset

# Lets look at the summary
summary(moody)

#Lets look at the number of rows in the dataset.
nrow(moody)

#Lets look at the number of columns in the dataset.
ncol(moody)

#Lets look at the dimensions i.e. both numbers of rows and columns of the data using just one command
dim(moody)

#Lets look at the mean of score column.
mean(moody$score)

#Lets look at the standard deviation of score column
sd(moody$score)

#Lets look at the length of the grade column 
length(moody$grade)

#Lets look at the minimum value of score in the score column.
min(moody$score)

#lets look at the maximum value of the score in the score column
max(moody$score)
```

---

## Table {#table}

- table() function in R Language is used to create a categorical representation of data with variable name and the frequency in the form of a table.
- More use of table() is when you use multiple categorical columns. For example, we'll see the count of *grade* vs *asks_questions* in example 2.

```{r,height=700}
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101_test/main/MOODY-2019.csv")


tableex1<- table(moody$GRADE) #Use of table  function on the new column.
tableex1
barplot(tableex1,col =c("red","purple","cyan","yellow","green"),xlab = "Labels", ylab = "Frequency",main = "table() example 1") #plot.


tableex2<-table(moody$GRADE,moody$ASKS_QUESTIONS)
tableex2
mosaicplot(tableex2,col =c("red","purple","cyan","yellow","green"),main = "table() example 2")
```

### Question What would R say?

```{r}
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101_test/main/MOODY-2019.csv")

table(moody[moody$ASKS_QUESTIONS!='always',]$GRADE)
#What will R say?

# A. error
# B. distribution of grades for students who always ask questions
# C. distribution of grades for students who do not always ask questions 
```
### Question What would R say?

```{r}
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101_test/main/MOODY-2019.csv")

table(moody[moody$ASKS_QUESTIONS==list('always','never'),]$GRADE)
#What will R say?

# A. error.
# B. distribution of grades for students who always or never ask questions.  
# C. distribution of grades for students who do not ask questions always or never. 
```


---

## Subset {#subset}

- subset() function in R programming is used to create a subset of vectors, matrices or data frames based on the conditions provided in the parameters.


- NOTE: To create a subset, not only can you use the subset() function, but also:
    - You can use [ ] operator. Ex: dataFrameName['columnName'] 
    - Even \$ operator is a subset operator. Ex: dataFrameName\$columnName
    
- Also, subsetting in R (commonly called “subscripting”) is done with square brackets.  When subscripting a data frame there will be two places inside the square brackets separated by a comma.
- The first part inside the square brackets corresponds to rows.  The second part corresponds to columns.
    
```{r}
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101_test/main/MOODY-2019.csv")


#Subset of rows
moody_never_smartphone<-subset(moody,ON_SMARTPHONE=="never")
nrow(moody)
nrow(moody_never_smartphone)
table(moody_never_smartphone$ON_SMARTPHONE) # You can see only student never on smartphone are in the subset.

#Alternate way to subset.
moody_never_smartphone_alt<-moody[moody$ON_SMARTPHONE=="never", ]
table(moody_never_smartphone_alt$ON_SMARTPHONE) # You can see a similar table as above.


#subset of columns
moody_except8<-subset(moody, select = -c(8))
ncol(moody)
ncol(moody_except8) # You can see the number of columns has been reduced by 1, due to subsetting without column 8

#Subset of Rows and Columns
moody_except8_never<-subset(moody, select = -c(8), ON_SMARTPHONE == "never")
table(moody_except8_never$ON_SMARTPHONE)
dim(moody)
dim(moody_except8_never)# You can see only student never on smartphones without column 8 data are present in the subset.


```


### Question What would R say?

```{r}
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101_test/main/MOODY-2019.csv")

moody[moody$SCORE>=90,3]
# What will R say?


# A. Get subset of all columns which contains students who scored more than equal to 90
# B. error
# C. get all score values which are more than equal to 90
# D. get subset of only the grades of students with score greater than equal to 90


```
### Question What would R say?

```{r}
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101_test/main/MOODY-2019.csv")

moody[moody$SCORE>=80.0 & moody$GRADE =='B',] 
# What will R say?

# A. subset of moody data frame who got B grade.
# B. error.
# C. subset of moody data frame with score greater than 80.
# D. subset of moody data frame with score more than 80 and got B grade.


```


---

## tapply {#tapply}

- tapply() function in R Language is used to apply a function over a subset of vectors given by a combination of factors
- This is a very versatile function, as we'll see from the use case. 
- Note : There are different aggregate functions that can be used. For example, Mean, Median, Variance, Sum etc.
-  We can also factor it on multiple attributes.

```{r,height=700}
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101_test/main/MOODY-2019.csv")


# To apply tapply() on SCORE factored on ON_SMARTPHONE

moody_scoreavg<-tapply(moody$SCORE,moody$ON_SMARTPHONE,mean)
moody_scoreavg # We can see it calculated mean value of the score by students with respect to their use of phone in class.

barplot(moody_scoreavg,col = "cyan",xlab = "Labels", ylab = "mean_val",main = "tapply() example 1",las = 2, cex.names = 0.75)#plot

#Lets factor the grades on on_smartphone as well as grade category.

moody.scoreavg2d<-tapply(moody$GRADE,list(moody$ON_SMARTPHONE,moody$GRADE),length)
moody.scoreavg2d[is.na(moody.scoreavg2d)]<-0
moody.scoreavg2d# We can see it calculated count of the grade of student with respect to their in-class smartphone usage  and grade category.
barplot(moody.scoreavg2d,col=c("red","cyan","orange","blue"),main = "tapply() example 2",beside = TRUE,legend=rownames(moody.scoreavg2d))
```


### Question What would R say?

```{r}
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101_test/main/MOODY-2019.csv")

tapply(moody, GRADE, SCORE, min)
# What will R say?

# A. minimum score for each grade
# B. minimum grade for each score
# C. minimum grade only 
# D. Error.


```
### Question What would R say?

```{r}
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101_test/main/MOODY-2019.csv")

tapply(moody$ASK_QUESTIONS, moody$GRADE, mean)
# What will R say?

# A. mean grade for each values of ask_question attribute
# B. mean value of ask_questions attribute for each grade
# C. mean category of ask_questions only 
# D. error.
```

---

## Cut {#cut}

- cut() function in R Language is used to divide a **numeric vector** into different ranges

```{r}
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101_test/main/MOODY-2019.csv")


# We access the Score column from moody dataset.
score0 <- cut(moody$SCORE,10)
table(score0) #lets check the distribution of people in each partition.

# Cut Example using breaks - Cutting data using defined vector. 
score1 <- cut(moody$SCORE,breaks=c(0,50,100),labels=c("F","P"))
table(score1)

```


### QuestionWhat would R say?

```{r}
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101_test/main/MOODY-2019.csv")

cut(moody$SCORE, breaks=c(0,25,70,100),labels=c("low", "medium", "high"))
#What would R say?

# A. 5 intervals of attribute score
# B. 3 intervals (0,25) (25,70) (75,100)
# C. 3 categorical values "low", "medium" and "high" for different score intervals
# D. 3 separate datasets with similar score values

```


### QuestionWhat would R say?


```{r}
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101_test/main/MOODY-2019.csv")

output<-cut(moody$SCORE, 5)
summary(output)
#What would R say?

# A. 5 intervals of attribute score of unequal count of elements
# B. 5 intervals of attribute score of equal count of elements
# C. 5 categorical values for different score intervals
# D. 5 separate dataset with similar score values

```


### QuestionWhat would R say?


```{r}
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101_test/main/MOODY-2019.csv")

output<-cut(moody$ASKS_QUESTIONS, 2)
summary(output)
#What would R say?

# A. 2 intervals of attribute ask_questions of unequal count of elements in each interval
# B. 2 intervals of attribute ask_questions of equal count of elements in each interval
# C. 2 categorical values for different ask_questions intervals
# D. Error.

```



---

### A complex example


```{r,height=700}

moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load dataset


moody$conditional <-0
moody[moody$participation<0.50, ]$conditional <- moody[moody$participation<0.50, ]$score -10*moody[moody$participation<0.50, ]$participation
moody[moody$participation>=0.50, ]$conditional <- moody[moody$participation>=0.50, ]$score +10*moody[moody$participation>=0.50, ]$participation

summary(moody$conditional)

boxplot(moody$conditional,col = c("red"),main="Complex Example")

```


--- 

## What would R say? {#basicexamples}

In this section we will look at few examples based on the question _"What do you think would R say?"_
All the questions are based on what we have studied in the sections above.

_INSTRUCTIONS_: Do not run the following examples directly, first ask yourself and note down, what *do you think* would R say?  Only then run them. This is the only way to learn simple commands - and have them memorized so you can write code without having to check every single command. 


### Question  

```{r,height=100}
weather =data.frame(Day=c('weekday', 'weekend'), Conditions =c('sunny','rainy','cloudy', 'snow', 'storm','ice'))
dim(weather)
#what would R say?

# A) 6 6
# B) 2 6
# C) 6 2
# D) Error
```

### Question 

```{r,height=150}
weather =data.frame(Day=c('weekday', 'weekend'), Conditions =c('sunny','rainy','cloudy', 'snow', 'storm','ice'))
weather$temperature =c(80, 70, 65, 40, 30,25)
weather[weather$temperature > 40,]
dim(weather)
#what would R say?

# A) 6 3
# B) subset of the dataframe with temperature > 40.
# C) Both A and B
# D) Error
```

### Question 

```{r,height=150}
SCORE=c(30,15,66);
GRADE=c('C', 'F', 'A')
ON_SMARTPHONE=c('always', 'never', 'sometimes')
FINALEXAM=c(12,5,20)
M=data.frame(SCORE, GRADE, ON_SMARTPHONE, FINALEXAM)
subset(M, GRADE=='F')
#what would R say?

# A) Subset of dataframe based on Grade equal to F
# B) Subset of the dataframe based on Grade not equal to F
# C) the complete dataframe
# D) Error

```

### Question 

```{r,height=150}
SCORE=c(30,15,66);
GRADE=c('C', 'F', 'A')
ON_SMARTPHONE=c('always', 'never', 'sometimes')
FINALEXAM=c(12,5,20)
M=data.frame(SCORE, GRADE, ON_SMARTPHONE, FINALEXAM)
M[FINALEXAM > 5,]
#what would R say?

# A) Subset of dataframe with finalexam values greater than equal to 6
# B) Subset of dataframe with finalexam values greater than equal to 5
# C) Subset of dataframe with finalexam values less than 5.
```

### Question 

```{r,height=150}
SCORE=c(30,15,66)
GRADE=c('C', 'F', 'A')
ON_SMARTPHONE=c('always', 'never', 'sometimes')
FINALEXAM=c(12,5,20)
M=data.frame(SCORE, GRADE, ON_SMARTPHONE, FINALEXAM)
M$QUESTIONS='none'
M[,5]
#what would R say?

# A) Output the content of all the columns
# B) Output word "none" for 3 times 
# C) Output word "none" for 5 times
# D) Error
```

### Question 

```{r,height=150}
SCORE=c(30,15,66)
GRADE=c('C', 'F', 'A')
ON_SMARTPHONE=c('always', 'never', 'sometimes')
FINALEXAM=c(12,5,20)
M=data.frame(SCORE, GRADE, ON_SMARTPHONE, FINALEXAM)
table(M$SCORE>15, M$GRADE)
#what would R say?

# A) Output the table of count of Score greater than 15 vs Grade
# B) Output the table of count of score greater than 15 only
# C) Output the table of count of grades only
# D) Output the table of grade distribution vs all score.


```

### Question 

```{r,height=100}
u<-c(1:10)
w <-c(1,-1,3)
u[w>0]
#what would R say?

# A)  1  3  4  6  7  9 10 
# B)  1  2  3  4  5  6  7  8  9  10
# C)  1  3  1  3  1  3  1  3  1  3 
# D) Error
```

### Question 

```{r,height=100}
v <- c(-2,0,2,-5)
v[v>0]
#what would R say?

# A) 2
# B) 0 2
# C) FALSE FALSE  TRUE FALSE
# D) Error
```

### Question 

```{r,height=100}
c("a",1,T)
#what would R say?

# A) NaN  1  NaN
# B) "a"  1  T
# C) "a"  "1"  "TRUE"
# D) "a"  "1"  "T"

```

### Question 

```{r,height=100}
x<-1:4
y<-2:9
x+y
#what would R say?

# A) 3  5  7  9  7  9 11 13
# B) 1  2  3  4  2  3  4  5  6  7  8  9
# C) 3  5  7  9
# D) Error
```



### Question

```{r}
v1<- c(1,2,3,4)
v2<- c(1,2,3,4)
v3<- c(1,2,3,4)

df<-data.frame(v1,v2,v3)
#1
df[df>2]  

# A. values of v1 variable which are larger than 2 

# B. values of v1, v2 and v3 which are larger than 2. 

# C. error 

```

### Question

```{r}
v1<- c(1,2,3,4)
v2<- c(1,2,3,4)
v3<- c(1,2,3,4)

df<-data.frame(v1,v2,v3)

df$v1 > 2 

# A. error 

# B. values of v1 variable which are larger than 2.

# C. TRUE where the value is greater than 2 and False where the value is less than 2. 

```

### Question

```{r}
v1<- c(1,2,3,4)
v2<- c(1,2,3,4)
v3<- c(1,2,3,4)

df<-data.frame(v1,v2,v3)

v1>2

# A. TRUE where the value is greater than 2 and False where the value is less than 2.

# B. values of v1 variable which are larger than 2

# C. error

```




<!--chapter:end:chapters/revcommands.Rmd-->

# Data Frames & Transformation. {#datatransformation}

<script src="files/js/dcl.js"></script>
```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

---

Now we have to introduce the core data structure of R – the data frame and show we can expand it with extra attributes.

Defining new attributes can very often be critical in data exploration and help to find patterns and relationships which otherwise would not be visible.

For example, may be participation matters but only to Pass/Fail grades? In other words students who Pass (A or B or C) always have participation above a certain threshold?  Perhaps students who always text never pass the class?  And students who always ask questions never fail?  Such rules can only be discovered if we define a new  Pass/Fail attribute, additional to grade attribute.


Similarly intervals of participation or score may discover important relationships which would not emerge with just numerical values of such attributes. May be High scores correlate with High participation? To establish it one would have first to define categorical attributes with named intervals of their numerical counterparts.

---

## Create Column

* Lets put a column I have created using score. 
* Suppose I am given a new column " pf " with same number of rows as that of the dataframe with the categories ("P" , "F").

```{r}
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load

# pf column has 2 category and divides on the basis of score.
pf <- cut(moody$score,breaks=c(0,50,100),labels=c("F","P"))
# length(pf) # Number of rows in new column.
# nrow(moody) # Number of Rows in dataframe

# To add this new column pf in dataframe moody.
names(moody) # Initially dataframe has 5 columns
moody$passfail <- pf #Put syntax dataFrameName$columnHeaderName <- newColumn
names(moody) # Now dataframe has 6 columns
```

```{r ,error=TRUE}

# moody<-read.csv("../files/dataset/moody2020b.csv") #static Load
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load

#What happens when you have column size mismatch.
badcol <- c(1:10)
length(badcol)

moody$badcol <- badcol #Throws Compatibility error. 
```

---


## Factor Function: factor() {#factor}

- Factors are the data objects which are used to categorize the data and store it as levels. 
- They can store both strings and numbers. 
- They are useful in the columns which have a limited number of unique values. Like "Male, "Female" and True, False etc. 
- Factor data objects are useful in data analysis for statistical modeling.

- The factor function is used to encode a vector as a factor.

Lets look at first example, checking if a data object is of factor type using the function _is.factor(x)_

```{r}
# Create a vector as input.
gender <- c("male","male","female","female","male","female","male")

gender

#Check if data object is factor.
is.factor(gender)

```

Now lets convert the above vector to a factor data object.
To do this we will use the function _factor(x)_.


```{r}
# Create a vector as input.
gender <- c("male","male","female","female","male","female","male")

# Apply the factor function.
factor_gender <- factor(gender)

factor_gender
is.factor(factor_gender)

```

Notice that for the factor data objects, the attribute _Levels_ is also created. This is an extremely important feature of the factor data object.


Lets look at how the factor data object looks when included in a dataframe.

```{r}
# Create the vectors for data frame.
height <- c(132,151,162,139,166,147,122)
weight <- c(48,49,66,53,67,52,40)
gender_not_factor <- c("male","male","female","female","male","female","male")

# COnvert the gender_not_factor vector to a factor data object.
gender <- factor(gender_not_factor)

# Create the data frame.
input_data <- data.frame(height,weight,gender)
print(input_data)

# Test if the gender column is a factor.
print(is.factor(input_data$gender))

# Print the gender column so see the levels.
print(input_data$gender)
```

Note: Sometimes depending on your version of R and packages, you might find that while inserting categorical vector into the data frame using the _data.frame()_ function, without converting the categorical vector to factor, it automatically gets converted into a factor column. But to avoid confusion, it is a better technique to convert the categorical vector into factor using _factor()_ function and then insert it in the data frame.


- Lets look at an example where the use of factor data object turns out to be useful.
  - We have a categorical vector that we want to coerce as numeric for use in some model/application.
  - Lets look at what happens when we just have a categorical vector, and we try to coerce it to numeric vector.
  - We see that the outcome of the _as.numeric()_ function on a normal categorical vector is coercion to "NA" of all elements.
  - But when we convert the same categorical vector to factor, then after coercion to numeric type, we get a numeric vector of elements corresponding the the index of the labels of the factor data object.
  
```{r}
gender_not_factor <- c("male","male","female","female","male","female","male")
gender_not_factor

# Coerce into numeric vector without converting to factor
as.numeric(gender_not_factor)


# COnvert the gender_not_factor vector to a factor data object.
gender <- factor(gender_not_factor)
gender

# Coerce into numeric vector after converting to factor data object.
as.numeric(gender)

```


- Lets look at another example where factor is useful.

  - We want to see the distribution of price of each quality for the wine dataset. 
  - Upon plotting, it gives us a scatter plot, which makes it hard for us to see the distribution.
  - Thus we convert the quality vector which is numeric initially, to factor and the plot it again.


```{r,height=600}
wine <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/541WINE.csv")
plot(wine$QUALITY,wine$PRICE)
#we want to see the distribution of price of each quality, but it gives us a scatter plot, which makes it hard for us to see the distribution.
is.factor(wine$QUALITY)
#the result is false, which means quality is a numeric value rather than a factor

factor_quality <- factor(wine$QUALITY)
#convert quality values into factors
plot(factor_quality,wine$PRICE)
#now we can generate the box plot and see the distribution clearly.
```


---

## Coercing Values in data frames {#coerce}

Before coercing data into data frames, lets look at small examples.

- Lets look at a coerced vector.


```{r}

#Lets look at a coerced vector.

#vector containing 4 elements
myVect<-c("Robert", "Ethan", 6, 4)
myVect

#You will notice that the last two elements , which are an integers, are coerced into a character type.

#class() is used to check the type of an object
class(myVect)


```

We see that when a vector has elements of mixed data types, they gets coerced into a type with precedence over other types. 
For example in the above case there were character elements and numeric elements types in the vector. But character type has precedence over numeric type and hence the whole vector is coerced into character type.


We can check the types of vectors using a specific type of _is_ function: _is.character(), is.double(), is.integer(), is.logical(),etc_. There are many other types under the _is_ function, for checking if the data object given is a dataframe, factor, etc. 

- Lets look at the examples.

```{r}

#vector containing 4 elements
myVect<-c("Robert", "Ethan", 6, 4)
myVect

# Check if vector is of Character type.
is.character(myVect)

# Check if vector is of numeric type.
is.numeric(myVect)

# Use TRUE and FALSE (or T and F) to create logical vectors
log_vec <- c(TRUE, FALSE, T, F)

# Check if vector is of logical type.
is.logical(log_vec)

```


We saw how to check the type of the data. But if you want to convert a column into your choice of data type, you can use the specific type of _as_ function: _as.character(), as.double(), as.integer(), as.logical(),etc._ Again as we saw above about the _is_ function types, there are also many other types of the _as_ function.

- Lets look at the example of coercing a vector into character type. 

<!-- Note: although the above myVect is a character vector, you can still use the function. -->
```{r}

#vector containing 4 elements
myVect<-c(2, 3, 6, 4, TRUE, FALSE)
myVect

# First lets look at the class of the vector
class(myVect)

# Coerce the vector to Character type. 
as.character(myVect) 

# You can see that the elements of the numeric vector are coerced into character type.
```

- Lets look at an example of coercing a mixed type vector into numeric type.

```{r}

myvec<-c("Robert", "22", 45)
myvec

# First lets look at the class of the vector
class(myvec)

# Coerce the character vector to numeric type. 
as.numeric(myvec) 

# You can see that the elements of the mixed vector are coerced into numeric type.



myvec2 <- c(TRUE, FALSE, F, T, T)
myvec2

# First lets look at the class of the vector
class(myvec2)

# Coerce the logical vector to numeric type. 
as.numeric(myvec2) 

# You can see that the elements of the mixed vector are coerced into numeric type.

```

We can see in the above example, while converting the character type vector to numeric if we encounter, numbers in character type, they get converted to numeric type. But the characters in character type, are not not converted, and instead we get a warning saying _"NAs introduced by coercion"_. 
Also, while converting a logical vector to numeric vector, we see that "TRUE" or "T" is coerced as _1_ and "FALSE" or "F" is coerced as _0_.


- Now lets look at how to coerce data column and rewrite it into the dataframe.

- Suppose in the Moody dataset, you want to change the categorical vector of letter grade to numeric grades between 1 to 5, where A=1, B=2, ..., F=5.
  - First, you will convert the grade column vector to factor using the _factor()_ function.
  - Then, convert the grade column with the command _as.numeric()_ to numeric column.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load dataset

# Convert the categorical column grade to factor data column.
moody$grade<-factor(moody$grade)
head(moody$grade)

# Now convert the levels to numeric using the as.numeric function
moody$grade <- factor(as.numeric(moody$grade))
head(moody$grade)
```

- We can see that the outcome of the above code, gives us a moody dataframe with grade column as a numeric column converted from the previous categorical column.
We can also see that the we used the as.numeric() function inside the factor function while converting from categorical to numeric, to maintain the levels information of the grade column.

- Now, suppose you also want to change the labels of the grade column. 
  - Lets change the grades from capital letters to small letters, i.e. A -> a, B -> b, and so on.
  - To do this, we can provide our user defined labels vector to the labels attribute of the _factor()_ function.
```{r}
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load dataset

# Convert the categorical column grade to factor data column with user defined labels.
moody$grade <- factor(moody$grade,labels = c("a","b","c","d","f"))
head(moody$grade)
```

We can see that the capital letter are now transformed to small letters.

---


## Merging Two Relational Data Frames. {#merge} 

Often, we have data from multiple sources/multiple databases, files etc. To perform analysis, we need to merge these dataframes together with one or more common key variables.

In R the  _merge()_ function allows merging two data frames by common columns or row names. This function allows you to perform different SQL joins, like `left join, inner join, right join or full join`, among others.

We will look at merging datasets in R with this function, along with examples.

Consider the following 2 datasets.

First is a smaller just 4 record data subset of the Moody dataset.  
```{r tut=FALSE,echo=FALSE}
library(knitr)
moodysm<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/SmallMoody.csv") #web load
temp<-knitr::kable(
  head(moodysm), caption = 'Small subset of Moody Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

Second is another dataset of students with respective GPA and Majors.

```{r tut=FALSE,echo=F}
library(knitr)
moodyst<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/SmallSt.csv") #web load
temp<-knitr::kable(
  head(moodyst), caption = 'Small dataset of students information',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

_NOTE:_ We can see from the above snippets of the above the top 3 records in both dataset have same `STUDENTID`, but the 4th records in both datasets are of different students. The most important element while discussing the examples below, will focus on what happens to the 4th records of both datasets when using the various merge options and attributes.


### Inner Join

This is the most usual type of join of datasets that you can perform. It consists of merging two dataframes in one that contains _common_ elements of both.

In order to merge the two datasets, you just have to pass them to the _merge()_ function without the need of changing other arguments. Inner join merge is the default merge of the _merge()_ function.

```{r}
# Import Datasets.
moody_df <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/SmallMoody.csv")
student_df <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/SmallSt.csv")

# Use the merge function, without any attributes.
merge(moody_df,student_df)
```
We can see that there are only 3 record in the output. The reason being that, the studentid of the fourth record in both the dataset did not match. And thus the merge function did not know which datasets record to be kept and which not.

Also the reason the merge function tried to match and merge the two datasets, is by using the first columns from both the datasets, which in both case was the _"STUDENTID"_ column. 

We can also do the same process, and get he same outcome, by defining the index column by yourself.
Lets look at this in the example below.

```{r}
# Import Datasets.
moody_df <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/SmallMoody.csv")
student_df <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/SmallSt.csv")

# Use the merge function, with the " by "  attribute.
merge(moody_df,student_df,by = "STUDENTID")
```

As we can see, the output remains the same. But we understand that we can define any other common column as the index column based on which the merging can occur.

**IMPORTANT NOTE:** There are also arguments like `by.x and by.y` which correspond to indexing based on one of the column from the left(first) or right(second) datasets respectively.
This could come extremely handy, when the two datasets you want to merge, have different column name for the index column.

For example, suppose in the two dataset that we have considered above, the first dataset had students records indexed by the `studentid` column where the indexing column name is `studentid`, but in the second dataset the indexing column even though with same student id's as entries but with the column name of `stu-id`.

Now while merging, you can face error since the _merge()_ function will have trouble finding the two index columns to match since they are named differently in the two datasets. 
Here you can provide the argument ` by.x = "studentid" , by.y = "stu-id" ` in the function while merging.

#### Another example
Suppose you have the happiness index dataset,

```{r tut=FALSE,echo=F}
library(knitr)
happy<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/HAPPINESS2017.csv") #web load
temp<-knitr::kable(
  head(happy), caption = 'Happiness Index Dataset for all countries',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

where you have the survey data of people of various countries with records of information about ` AGE, COUNTRY, GENDER, IMMIGRANT, INCOME, and HAPPINESS`.

You can do analysis on the above dataset per country, per age group,etc. 

But if you want to do analysis based on per continent, then you will have to create lists of all the countries in each continent, and then subset using the appropriate subset method/s from section below \@ref(sliceanddice).

Alternate method will be acquiring another dataset, with information of each country and its respective continent, and do merge, which we can then use to subset easily.

Lets look at an example of this process below.

```{r}
# Import the Happiness index dataset.
happy<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/HAPPINESS2017.csv")
head(happy)

# Now lets load the simple dataset of country and continents.
continents<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/country-continents.csv")
head(continents)

# Now we can use the merge function to include the continents of each country in the happiness dataset against each other.
happy.c<-merge(happy,continents)
head(happy.c)

happy.c[sample(nrow(happy.c),10),]


```
We can see from the output of the above example, the new dataframe created in `happy.c` after applying _merge()_ function on the Happiness index dataset and the country-continents dataset, the `CONTINENT` column is added from the country-continents dataset into the happyness index dataset. And each country in the `happy.c` dataframe has now the value of its respective continent in the the `CONTINENT` column.


### Full Join

Full Join is also known as the `outer join` or the full outer join.
It merges all the columns of both datasets into one. 

For those records with non-intersecting index elements, Full join keeps both the records, and fills the missing values with NA , i.e. Not Available(NA) keyword.

In order to create this type of full join of the two dataframes in R, we need to set the argument `all` to `TRUE or T`.
Lets look at this in the example below.


```{r}
# Import Datasets.
moody_df <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/SmallMoody.csv")
student_df <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/SmallSt.csv")

# Use the merge function, with the " all "  attribute.
merge(moody_df,student_df,all = TRUE)
```

We can see that the first record of the output with `studentid = 10001 ` was present in the second dataset only, thus the values corresponding to the columns of the first dataset are set to `NA`. 
Similarly, the same occurs with the record with `studentid = 16792  `, which was only present in the first dataset, and thus has `NA` in the place of columns of second dataset.


### Left Join

The left join in R involves matching all the rows in the first data frame with the corresponding records on the second dataframe. 

To create this left join, you just have to set the argument ` all.x` to ` TRUE or T`.

Recall while doing the full join, we set the argument `all` to ` TRUE or T`. Similarly, since we consider `x` as the first dataset or the left dataset, we will set the argument of `all.x` where the `.x` is the key to select the first dataset.

We have seen in the snippets above, the student with ` studentid = 16792` is only present in the first dataset but not the second. 

So lets see the result of merging using the left join in the example below.


```{r}
# Import Datasets.
moody_df <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/SmallMoody.csv")
student_df <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/SmallSt.csv")

# Use the merge function, with the " all "  attribute.
merge(moody_df,student_df,all.x = TRUE)
```
We can see that the record of student with `studentid = 16792` has `NA` as the entry in the columns merged from the right dataset. Also, the record of student with `studentid = 10001` is completely excluded, since it belongs to the second dataset.

### Right Join

The right join merge involves joining all the rows in the second data frame with the corresponding records on the first dataframe.

The right join is opposite to that of left join.

In consequence, here, you will need to set the argument ` all.y` to `TRUE or T`, since we consider the right dataset or the second dataset as `y`.

We have seen in the snippet above, that the student with `studenid = 10001` is only present in the second dataset but not the first. 

So lets see the result of merging using the right join in the example below.

```{r}
# Import Datasets.
moody_df <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/SmallMoody.csv")
student_df <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/SmallSt.csv")


# Use the merge function, with the " all "  attribute.
merge(moody_df,student_df,all.y = TRUE)
```
We can see that the record of student with `studentid = 10001` has `NA` as the entry in the columns merged from the left dataset. Also, the record of student with `studentid = 16792` is completely excluded, since it belongs to the first dataset.


---


## Slicing and Dicing.{#sliceanddice}


R was made especially for data analysis and graphics.  SQL was made especially for databases.  They are allies in this field of data science.

The data structure in R that most closely matches a SQL table is a data frame.  The terms rows and columns are used in both.

There is an R package called `sqldf` that allows you to use SQL commands to extract data from an R data frame.  We will not use this package in the examples but look at a way the operations in SQL translate to basic R commands that we have studied in previous chapter \@ref(revision).

In R we have seen how subsetting of rows and columns happen using the subset function in earlier chapters \@ref(subset).
Please review this section before proceeding ahead.

### Subsetting on Columns ( DICING ) {#dicing}

So lets start with dicing the dataframe. In other words, lets look at subsetting operations on columns.

Columns in SQL are also called “fields”.  In R it is commonly called “variables”.

In SQL the subset of columns is determined by `SELECT` statement.

We can do these type of SQL operation in R using the normal subsetting method, either using the _subset()_ function or using the square brackets ` [ ] `. 

_NOTE_: In most of the examples below, to avoid printing of the complete dataset after any operations, we have used the _head()_ function to truncate the output to only top 6 rows. However you can always remove the function or change the limit or output records to your choice by passing additional attribute  ` n = user_defined_limit ` to the _head()_ function.


Just to recap subsetting on columns,

#### Subset single column.

**Remember:** You can either use the column names or the column location index, to dice the dataframe.

Suppose we want to subset the moody dataset only the grade column.
Lets look at this example.
```{r}
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load dataset

# Lets subset the grade column form the moody dataset and look at its first few elements..
head(moody[,'grade',drop=F])

# Without `drop=F` in the attribute, you will get only the values of the column.
head(moody[,'grade'])
```

We can see that only one column is selected form the dataframe. The `drop = F` attribute is provided to keep the dataframe structure. You can also see the effect of not using the `drop = F` attribute in the above example.
**Note**: In some cases, where you want to use the subsetted column with other function, e.g. `mean(subsetted_column)` you must not use the `drop=F` attribute, otherwise it will result in error.


#### Subset multiple column.

Suppose you want to subset multiple columns by name, you can create a vector or the column names you want to subset and then include it wile subsetting.

Lets look at the example.
```{r}
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load dataset

# here we create a vector of column name that we want to subset.
columnNames<- c("grade","score")

# Including the above vector wile subsetting.
head(moody[,columnNames])
```
We can see that only the two column of "grade" and "score" are kept in the subset.
Similarly, we can include the multiple column names and get subset.

#### Subset on all columns
You can get all the columns in the subset, by keeping the space after the comma blank. This gives the complete set of columns.

Suppose you did some slicing on the dataframe and want to keep all the columns in the output, you can just keep the space after the comma blank while subsetting.
```{r}
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load dataset

# here we can see that we sliced the dataframe to only keep the records of students with grade "A". 
head(moody[moody$grade=="A", ])


# We  will look at slicing in the subsequent sections.

```





### Subsetting on Rows ( SLICING ) {#slicing}

Now that we have seen dicing Or subsetting on columns, which is similar to the `select` statement of SQL,
we will now look at slicing on the dataframe. Or in other words subsetting on rows.

There are many statements of SQL that does subsetting on rows, i.e. `SELECT, WHERE, AND, OR, IN, LIKE, LIMIT`, and many more.
We will look at few of them, by implementing them using the basic R functions.


#### Subsetting based on single condition.

We will look at a subsetting condition based on value.

For subsetting based on value, you can use the relational operators e.g. _> , < , >= , <= , == , etc_ between the attribute name and the value.

Lets look at this in the following example.

Suppose you want to keep all the observations of where score of students are greater than 80.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load dataset

# here we can see that we sliced the dataframe to only keep the records of students with score greater than 80. 
head(moody[moody$score>80, ])

```
We can see from the above result, the subset has only records of students having score greater than 80.
This example is similar to using `where` statement in SQL.


#### Subsetting based on multiple conditions.

Similar to the above example, suppose you want to subset based on multiple conditions.

To do this, we will use the logical operators e.g. ` AND (" & ") , OR (" | ") , NOT (" ! ") ` between the various conditions.

Lets look at an example for this type of subsetting.

Suppose you want to slice the records of the moody dataset, based on two conditions:
- Students with grade equal to " A " 
- AND
- Students with score greater than 90.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load dataset

# here we can see that we sliced the dataframe to only keep the records of students with score greater than 90 AND with grade equal to "A" . 
head(moody[moody$score>90 & moody$grade == c("A"), ])

```

We can see that the records of students with score greater than 90 and grade equal to A are kept, rest all records are removed.

This example is similar to using the ` AND, OR, NOT ` clause in SQL.

#### Subset based on multiple values.

We will look at subsetting the dataframe based on one condition with multiple values.

Suppose you want to subset the moody dataset, based on the students grade, but you want to keep students records with grade equal to both "B" and "C"

Well you can use multiple conditions as seen above with an AND clause between the two conditions with different values on the same variable/columns, but there is a simple and useful way to do this with just one conditional statement.

We will make use of a vector of all the values that we want to use, and then assign this vector to the condition statement.

Lets look at this in the following example.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load dataset

# lets create a vector of values required in the conditional statement
condValues<- c("B","C")

# here we can see that we sliced the dataframe to only keep the records of students with grade equal to "B" or "C" . 
head(moody[moody$grade == condValues, ])
unique(moody[moody$grade == condValues, ]$grade)

# we can also directly write the vector without assigning a variable.
head(moody[moody$grade == c("B","C"),])
unique(moody[moody$grade == c("B","C"),]$grade)


```


We can see that the output has only records of students with grades B or C.
And both the methods, result in same output.
This example is similar to the  `IN` operator of the SQL

#### Subset based on a partial/complete text/character.

We will look at subsetting the dataset based on a specific pattern of text/characters.

This type of subsetting proves useful in text columns where each record has one or more than one sentence, and you want to search for a particular keyword or pattern.

Most simple example would be of a survey dateset, where each record in the dataset consists of text paragraph, answering the questions asked in the survey, and you want to figure out the count of particular keywords in each response.

Lets look at an example based on the Happiness dataset.
We would like to find the subset of countries with the letters " and " in their name. eg. Iceland, Uganda, Poland, etc.

```{r}
happy<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/HAPPINESS2017.csv") #web load dataset

# Subset using the grep function to find the pattern "and" in the names of the countries
head(happy[grep("and",happy$COUNTRY,ignore.case = T),])
unique(happy[grep("and",happy$COUNTRY,ignore.case = T),]$COUNTRY)
```

We can see the output has subset of the happy dataset with records of only those countries with the pattern "and" in its name.
To do this we have used the _grep()_ function, which is a really important function for finding patterns in text and data.
We don't need to study this _grep()_ function in detail, but one can find very good resources explaining it online.
This example is similar to the `LIKE` operator in SQL.




## Group By

<!-- The reason we call this section as Psuedo Equivalent of Group By is the fact that there are no direct functions that combine select ans Group -->

Now that we have done Slicing and Dicing, we will like to apply some functions and gain measured information form the subsets.

Although there is no straightforward, direct/ one step function to perform the function as that of the ` GROUP BY` from SQL, but we can get the required functionality, by combining various functions step by step from the R.7 commands list \@ref(freestyle) and the things we learned in this section \@ref(datatransformation) and the revision section \@ref(revision).

This section will involve use of the _table(), tapply()_ function to apply the functions like `mean, count, sum, etc` on the subsets categorical or numerical columns. More importantly, we will look at a very useful example below, which will tie together all that we have learned until now.

Suppose you want to get the statistics/numbers of average scores per grade and frequency of students per grade, and then use this table afterwards. 

So the SQL query will look something like `SELECT grade, avg(score) as averagescore,  count(*)  as student_number FROM moody GROUP BY grade`.

To implement this above query functionality in R we would fisrt need to use the tapply function to find get the average score per grade and frequency per grade, and then combine it. Lets look at this process in the code below.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv") #web load dataset


# Create a table of frequency of students per grade.
grade.count <- tapply(moody$grade,moody$grade,length)

# Create a table of average of students score per grade.
grade.mean <- tapply(moody$score,moody$grade,mean)

# We now combine the two tables together using cbind and store it as data.frame for simple post-processing.
out<-as.data.frame(cbind(grade.count,grade.mean))
out

```
We can see the combined table of both the average scores and frequency per grade.

Now suppose we want to go one step ahead and want to order the `out` table from the example above, based on decreasing value of frequency of students per grade.

To do this, we introduce the _order()_ function.

- __Order() Function__
  - The order() function returns a permutation of the order of the elements of a vector.
  - You can decide by passing the argument to order the elements in ascending or descending order.

An important thing to note, is that for our use for the example we discussed above, we will use the order function as a subset parameter. Lets look at this in the example below.
```{r}
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv")
grade.count <- tapply(moody$grade,moody$grade,length)
grade.mean <- tapply(moody$score,moody$grade,mean)
out<-as.data.frame(cbind(grade.count,grade.mean))

out

# Now lets order the out data, based on the grade.count column, in ascending order.
out[order(out[,'grade.count']),]

# If you want the output in descending order just pass 'TRUE' or 'T' the  decreasing argument of the order function.
out[order(out[,'grade.count'],decreasing = T),]

```
We saw how we can use implement ordering in R. This is similar to using the `ORDER BY` statement of SQL.

Another thing we can do is subsetting on the output. 
Suppose you want to keep only those grade records in the `out` data with frequency of students greater than 150 for particular grade.

To do this we will use the technique studied in the slicing section \@ref(slicing).
Lets look at the working of the above example.

```{r}
moody<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/moody2020b.csv")
grade.count <- tapply(moody$grade,moody$grade,length)
grade.mean <- tapply(moody$score,moody$grade,mean)
out<-as.data.frame(cbind(grade.count,grade.mean))

out

# To keep the records where frequency of students in particular grade is greater than 150.
out[out$grade.count>150,]


```
We see that the `B` Grade had only 108 students in the record, it is removed from the `out` dataframe.
This is similar to using the `HAVING` clause of SQL.



## Handling Date and Time in dataframes.

one of the most common issue that a novice or even an experienced R user can face is of handling date and time information available into the dataset, and importing it to use as a variable that is appropriate ans usable during analysis.

Also getting R to agree that your data contains the dates and times can be tricky sometimes. We will see an example where the usual R data import fails to read date and time as actually date and time.

To simplify this issue, we use a package called `lubridate`, which makes it easier to work with dates and times and converts them into POSIXct format. POSIXct is a class of data recognized by R as being a date or date and time. Lubridate's functions handle wide variety of formats and separators, which simplifies the parsing process.


Lets look how easy it is to use it and convert date and time input to be used in analysis.
First we will look at converting date in character format to POSIXct.

- First we will convert ` "20200317" `which is in year-month-date format. To convert this we will use the `ymd()` function of the `lubridate` package.
- Second we will convert ` "03-17-2020" `which is in month-date-year format. To convert this we will use the `mdy()` function of the `lubridate` package. 
- Third we will convert ` "17/03/2020" `which is in date-month-year format. To convert this we will use the `dmy()` function of the `lubridate` package.

```{r}
library(lubridate) # include the lubridate library.

# First we use the ymd() function.
ymd("20200317")

# Second we use the mdy() function.
mdy("03-17-2020")

# Third we use the dmy() function.
dmy("17/03/2020")


```
We can see the output of all the 3 function is the same, this means that the functions used have successfully converted all the input character type dates into the standardized POSIXct data type.


Now lets look at converting time in character format to POSIXct.

- First we will convert ` "18:20" ` which is in hour-minutes format. To convert this we ill use the `hm()` function of the `lubridate` package.
- Second we will convert ` "18:20:30" ` which is in hour-minute-second format. To convert this we ill use the `hms()` function of the `lubridate` package.

```{r}
library(lubridate) # include the lubridate library.

# First we will use the hm() function.
hm("18:20")

# Second we will use the hms() function.
hms("18:20:30")
```
We can see that the output of the 2 functions above are in POSIXct format and has the information of hours minutes and seconds annotated properly.

There are various other functions in the lubridate package like for various use case, but we will not cover them since they are not useful here. To learn more about it you can visit the official `lubridate` package vignette linked here: [lubridate](https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html) 


Now coming back to the main example of avoiding issues/errors while importing date and time attributes present in dataset. For this we will look at the `AirQualityUCI` dataset. And here is the snippet of the dataset below.

```{r tut=FALSE,echo=F}
library(knitr)
aq<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/AirQualityUCI.csv") #web load
temp<-knitr::kable(
  head(aq[,1:6]), caption = 'Air Quality Dataset of amount of elements and pollutants in air.',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

You will see from the dataset that the date and time columns are imported correctly. But in fact, and as we will see in the code below, the date column is of type `character` and the time is also of type `character`.

Now to convert these columns into POSIXct supported date time columns we will use the lubridate functions.
And then we will count the number of records in the dataset per _year_ using the `year()` function.

```{r}
aq<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/AirQualityUCI.csv") #web load
aq<-aq[,1:6] # Reducing the number of columns.

head(aq)

# Lets look at the type of the Date column after importing the dataset.
class(aq$Date) # Date Column

# Now lets use the mdy() function which converts the month-day-year format to POSIXct format.
aq$Date<-mdy(aq$Date)
head(aq)

# Now lets check the type of the Date column again.
class(aq$Date)


# Lets create a frequency table for the frequency of records per year using the table() and year() function
table(year(aq$Date))
```
We can see that the original type of the date column was `"character"` but then after using the lubridate's function, we converted it to a suitable POSIXct format of `"Date"`. Then we were easily able to subset the dataset based on the year, and get the frequency count of the records per year, as seen from the table for the years ` 2004 and 2005`.

Similarly, we can also convert the time column and probably use it later in analysis process.

```{r}
aq<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/AirQualityUCI.csv") #web load
aq<-aq[,1:6] # Reducing the number of columns.

head(aq)

# Lets look at the type of the Time column after importing the dataset.
class(aq$Time) # Time Column

# Similarly for the time column lets use the hms() function.
aq$Time<-hms(aq$Time)

# Now lets check the type of the Time column again.
class(aq$Time)
```
We can see that the original type of the time column was `"character"` but then after using the lubridate's function, we converted it to a suitable POSIXct format of `"Period"` which is used to represent time information.



**EOC**










<!-- --- -->

<!-- ## Table -->

<!-- - table() function in R Language is used to create a categorical representation of data with variable name and the frequency in the form of a table. -->

<!-- ```{r} -->
<!-- tableex1<- table(mdy$GRADE) #Use of table  function on the new column. -->
<!-- tableex1 -->
<!-- barplot(tableex1,col =c("red","purple","cyan","yellow","green"),xlab = "Labels", ylab = "Frequency",main = "table() example 1") #plot. -->
<!-- ``` -->


<!-- - More use of table() is when you use multiple categorical columns. For example, lets see the count of *grade* vs *asks_questions*. -->

<!-- ```{r} -->
<!-- tableex2<-table(mdy$GRADE,mdy$ASKS_QUESTIONS) -->
<!-- tableex2 -->
<!-- mosaicplot(tableex2,col =c("red","purple","cyan","yellow","green"),main = "table() example 2") -->
<!-- ``` -->






<!-- --- -->

<!-- ## Subset -->

<!-- - subset() function in R programming is used to create a subset of vectors, matrices or data frames based on the conditions provided in the parameters. -->

<!-- - NOTE: To create a subset, not only can you use the subset() function, but also: -->
<!--     - you can use [ ] operator. Ex: dataFrameName['columnName']  -->
<!--     - Even \$ operator is a subset operator. Ex: dataFrameName\$columnName -->

<!-- ```{r} -->
<!-- #Subset of rows -->
<!-- mdy_never_smartphone<-subset(mdy,ON_SMARTPHONE=="never") -->
<!-- nrow(mdy) -->
<!-- nrow(mdy_never_smartphone) -->
<!-- table(mdy_never_smartphone$ON_SMARTPHONE) # You can see only student never on smartphone are in the subset. -->

<!-- #subset of columns -->
<!-- mdy_except8<-subset(mdy, select = -c(8)) -->
<!-- ncol(mdy) -->
<!-- ncol(mdy_except8) # You can see the number of columns has been reduced by 1, due to subsetting without column 8 -->

<!-- #Subset of Rows and Columns -->
<!-- mdy_except8_never<-subset(mdy, select = -c(8), ON_SMARTPHONE == "never") -->
<!-- table(mdy_except8_never$ON_SMARTPHONE) -->
<!-- dim(mdy) -->
<!-- dim(mdy_except8_never)# You can see only student never on smartphones without column 8 data are present in the subset. -->
<!-- ``` -->


<!-- --- -->

<!-- ## tapply -->

<!-- - tapply() function in R Language is used to apply a function over a subset of vectors given by a combination of factors -->
<!-- - This is a very versatile function, as we'll see from the use case.  -->
<!-- - Note : There are different aggregate functions that can be used. For example, Mean, Median, Variance, Sum etc. -->

<!-- ```{r} -->
<!-- # To apply tapply() on capital gains factored on mdydiac sign. -->

<!-- mdy_scoreavg<-tapply(mdy$SCORE,mdy$ON_SMARTPHONE,mean) -->
<!-- mdy_scoreavg # We can see it calculated mean value of the score by students with respect to their use of phone in class. -->

<!-- barplot(mdy_scoreavg,col = "cyan",xlab = "Labels", ylab = "mean_val",main = "tapply() example 1",las = 2, cex.names = 0.75)#plot -->
<!-- ``` -->

<!-- - We can also factor it on multiple attributes. -->
<!-- - Lets factor the grades on on_smartphone as well as grade category. -->


<!-- ```{r} -->

<!-- mdy.scoreavg2d<-tapply(mdy$GRADE,list(mdy$ON_SMARTPHONE,mdy$GRADE),length) -->
<!-- mdy.scoreavg2d[is.na(mdy.scoreavg2d)]<-0 -->
<!-- mdy.scoreavg2d# We can see it calculated count of the grade of student with respect to their in-class smartphone usage  and grade category. -->
<!-- barplot(mdy.scoreavg2d,col=c("red","cyan","orange","blue"),main = "tapply() example 2",beside = TRUE,legend=rownames(mdy.scoreavg2d)) -->
<!-- ``` -->


<!-- --- -->

<!-- ## Cut -->

<!-- - cut() function in R Language is used to divide a **numeric vector** into different ranges -->

<!-- ```{r} -->

<!-- # We access the Score column from moody dataset. -->
<!-- score0 <- cut(mdy$SCORE,10) -->

<!-- levels(score0) # Lets make sure that the cut really create how many partitions.  -->
<!-- table(score0) #lets check the distribution of people in each partition. -->

<!-- # Cut Example using breaks - Cutting data using defined vector.  -->
<!-- score1 <- cut(mdy$SCORE,breaks=c(0,50,100),labels=c("NC","P")) -->
<!-- table(score1) -->

<!-- # Example using pretty - Cutting the numerical data into categories. -->
<!-- score2<- cut(mdy$SCORE,pretty(mdy$SCORE,2),labels=c("NC","P")) -->
<!-- table(score2) -->

<!-- ``` -->


<!-- --- -->

<!-- ### Some helpful functions -->
<!-- * pretty() function in R Language is used to decide sequence of equally spaced round values. -->

<!-- ```{r} -->
<!-- pretty(1:50,n=5) #The values are chosen so that they are 1, 2 or 5 times a power of 10. -->
<!-- ``` -->

<!-- * seq() function in R Language is used to create a sequence of elements in a Vector. It takes the length and difference between values as optional argument. -->

<!-- ```{r} -->
<!-- seq(2,10,2) -->
<!-- ``` -->

<!--chapter:end:chapters/datatransformation.Rmd-->

# Data Modeling and Prediction techniques for Classification. {#classification}

<script src="files/js/dcl.js"></script>
```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```

## Decision Tree. {#decisiontree}

Decision tree is one of the most powerful and popular tool for classification and prediction. 
It is a supervised learning predictive model that uses a set of binary rules to calculate a target value.
It is a flow chart like tree structure, where each internal node has a test on a particular attribute, each branch denotes the outcome of the test, and each leaf node holds a class label/ numeric value.

The reason decision tree are very popular are:
- It is able to generate rules easier to understand as compared to other models..
- It require much less computations for performing modeling and prediction.
- Both continuous/numerical and categorical variables are handled easily while creating the decision trees.

There are a few drawbacks too while using decision trees in certain case of inputs and tasks. But for the scope of discussion of this course we won't go into much details of it.
Also, we won't go into the depth of the internals of the formation/creation of the decision tree and its underlying algorithm, but we will look at decision tree as a way to create prediction model for both classification and regression.

---

## Use of Rpart {#rpart}

Recursive Partitioning and Regression Tree `RPART` library is a collection of routines which implement Classification and Regression Tree (CART) which is a type of Decision Tree.The resulting model can be represented as a binary tree.

The library associated with this `RPART` is called `rpart`. Install this library using `install.packages("rpart")`.

Syntax for building the decision tree using rpart():

- `rpart( formula , method, data, control,...)`
  - *formula*: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. 
    - `prediction ~ predictor1 + predictor2 + predictor3 + ...`
  - *method*: here we describe the type of decision tree we want. If nothing is provided, the function makes an intelligent guess. We can use "anova" for regression, "class" for classification, etc.
  - *data*: here we provide the dataset on which we want to fit the decision tree on.
  - *control*: here we provide the control parametes for the decision tree. Explained more in detail in section further in this chapter.
  
  
For more info on the rpart function visit [rpart documentation](https://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/rpart)

Lets look at an example on the Moody 2019 dataset.

- We will use the rpart() function with the following inputs:
  - prediction -> GRADE
  - predictors -> SCORE, ON_SMARTPHONE, ASKS_QUESTIONS, LEAVES_EARLY, LATE_IN_CLASS
  - data -> moody dataset
  - method -> "class" for classification.

```{r,height=700}
library(rpart)
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv")

# Use of the rpart() function.
rpart(GRADE ~ SCORE+ON_SMARTPHONE+ASKS_QUESTIONS+LEAVES_EARLY+LATE_IN_CLASS, data = moody[,-c(1)],method = "class")

```
We can see that the output of the rpart() function is the decision tree with details of, 

- node -> node number
- split -> split conditions/tests
- n -> number of records in either branch i.e. subset
- yval -> output value i.e. the target predicted value.
- yprob -> probability of obtaining a particular category as the predicted output.

Using output tree, we can use the predict function to predict the grades of the test data. We will look at this process later in section \@ref(rpartpredict)

But coming back to the output of the rpart() function, the text type output is useful but difficult to read and understand, right! We will look at visualizing the decision tree in the next section.


---

## Visualize the Decision tree {#rpartplot}


To visualize and understand the rpart() tree output in the easiest way possible, we use a library called `rpart.plot`. The function `rpart.plot()` of the rpart.plot library is the function used to visualize decision trees.

The `rpart.plot` library is a front-end wrapper to the library `prp` which is the most basic library for plotting decision trees. `prp` allows various aesthetic modifications for visualizing the decision tree. We will look at a few examples of using `prp` below.

But, first lets look at a example to visualize the output decision tree in the previous example on Moody dataset using `rpart.plot()`

*NOTE*: The online runnable code block does not support `rpart.plot and prp` library and functions, thus the output of the following code examples are provided directly.

```{r, tut=FALSE,eval=FALSE}
# First lets import the rpart library
library(rpart)

# Import dataset
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv")

# Use of the rpart() function.
tree <- rpart(GRADE ~ SCORE+ON_SMARTPHONE+ASKS_QUESTIONS+LEAVES_EARLY+LATE_IN_CLASS, data = moody,method = "class")

# Now lets import the rpart.plot library to use the rpart.plot() function.
library(rpart.plot)

# Use of the rpart.plot() function  to visualize the decision tree.
rpart.plot(tree)
```
![Output Plot of *rpart.plot()* function](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling/rpartplot1.svg)

We can see that after plotting the tree using rpart.plot() function, the tree is more readable and provides better information about the splitting conditions, and the probability of outcomes. Each leaf node has information about 

- the grade category.
- the outcome probability of each grade category.
- the records percentage  out of total records.

To study more in detail the arguments that can be passed to the rpart.plot() function, please look at these guides [rpart.plot](https://www.rdocumentation.org/packages/rpart.plot/versions/3.0.9/topics/rpart.plot) and [Plotting with rpart.plot (PDF)](http://www.milbo.org/doc/prp.pdf)

Note that for any beginner using rpart.plot() function is the easiest way. But if you want to learn another way of plotting rpart trees then the following function can be used.

So,another form of plotting rpart trees in a very minimalistic way is using the `plot rpart i.e. prp()` function, which is actually the working function behind rpart.plot().

Lets look at a same example like above but using prp().

```{r, tut=FALSE,eval=FALSE}
# First lets import the rpart library
library(rpart)

# Import dataset
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv")

# Use of the rpart() function.
tree <- rpart(GRADE ~ SCORE+ON_SMARTPHONE+ASKS_QUESTIONS+LEAVES_EARLY+LATE_IN_CLASS, data = moody[,-c(1)],method = "class")

# Now lets import the rpart.plot library to use the rpart.plot() function.
library(rpart.plot)

# Use of the prp function  to visualize the decision tree.
prp(tree)
```

![Output Plot of *prp()* function](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling/prp1.svg)

We can see that the output of the prp() function is a very minimalist tree, without any colors with minimum required information. There are other arguments that can be passed to the prp() function to increase the aesthetic look and the information provided. To learn those extra arguments visit this guide [prp()](https://www.rdocumentation.org/packages/rpart.plot/versions/3.0.9/topics/prp)

---
**NOTE**: In this chapter, from this point forward, the rpart.plots() generated in any example below will be shown as images, and also the code to generate those rpart.plots will be commented in the interactive code blocks. If you want to generate these plots yourself, please use a local Rstudio or R environment.
---

## Rpart Control {#rpartcontrol}

We will now look at the control argument used in `rpart()` function, which is one of the important argument. The control argument of rpart() function is used to manually decide the control parameters of the decision tree.

The advantages of using control method:
- It restricts the height of the decision tree.
- It avoids overfitting on the training dataset.
- It can be used to eliminate attributes that affect less significantly on the splitting constraints.
- Helps to terminate the creation process of tree earlier, thus reducing required computational time.


The disadvantages of using control method:
- It creates risk of generating trees with lesser accuracy compared to uncontrolled tree.
- It could hamper the splitting condition selection in negative way.
- Could result in underfitting, if control parameters not chosen carefully.

As we can see, that controlling the decision tree provides us with lot of advantage in certain condition, we also risk in reducing the accuracy of the prediction from using the tree.

Thus these control methods must be applied only in certain case, where the uncontrolled method takes large amount of time to create the tree, and overfits the train data. When the datasets have more significantly higher count of columns but less records of data, using control methods could be suitable.

Now lets look at the rpart.control() function used to pass the control parameters to the control argument of the rpart() function.

- `rpart.control( *minsplit*, *minbucket*, *cp*,...)`
 - *minsplit*: the minimum number of observations that must exist in a node in order for a split to be attempted. For example, minsplit=500 -> the minimum number of observations in a node must be 500 or up, in order to perform the split at the testing condition.
 - *minbucket*: minimum number of observations in any terminal(leaf) node. For example, minbucket=500 -> the minimum number of observation in the terminal/leaf node of the trees must be 500 or above.  
 - *cp*: complexity parameter. Using this informs the program that any split which does not increase the accuracy of the fit by *cp*, will not be made in the tree.
 

For more information of the other arguments of the `rpart.control()` function visit [rpart.control](https://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/rpart.control)

Note: The ratio of minsplt to minbucket is 3:1. Thus if only one of the minsplit/minbucket is provided the other value is set using the above ratio. Also if both values are provided, unless the values are not in the above ratio, the rpart.control() the resorts to the default value. Also note, the default value of cp is `0.01`.

Let look at few examples.

Suppose you want to set the control parameter minsplit=200. 

```{r}
# library(rpart)
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv")

# Use of the rpart() function with the control parameter minsplit=200
tree <- rpart(GRADE ~ ., data = moody[,-c(1)],method = "class",control=rpart.control(minsplit = 200))

# Check the count of observation at each split test. To do this we find the count at each non-leaf/non-terminal node.
tree$frame[tree$frame$var!="<leaf>",c("var","n")]

# library(rpart.plot)
# rpart.plot(tree,extra = 2)
```
![Output tree plot of after setting minsplit=200 in rpart.control() function](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling/rcontrolsplit.svg)

We can see from the output of `tree$splits` and the tree plot, that at each split the total amount of observations are above 200. Also, in comparison to the tree without control, the tree with control has lower height, and lesser count of splits.

Now, lets set the minbucket parameter to 100, and see how that affects the tree parameters.


```{r}
library(rpart)
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv")

# Use of the rpart() function with the control parameter minsplit=200
tree <- rpart(GRADE ~ ., data = moody[,-c(1)],method = "class",control=rpart.control(minbucket = 100))

# Check the count of observation in each leaf node.
tree$frame[tree$frame$var=="<leaf>",c("var","n")]

# library(rpart.plot)
# rpart.plot(tree,extra = 2)

```
![Output tree plot of after setting minbucket=100 in rpart.control() function](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling/rcontrolbucket.svg)

We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 100. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size.

Lets now use the `cp` parameter and see its effect on the tree.

```{r}

library(rpart)
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv")

# Use of the rpart() function with the control parameter cp=0.005
tree <- rpart(GRADE ~ ., data = moody[,-c(1)],method = "class",control=rpart.control(cp = 0.005))

# Check the accuracy increase factor at each split.
tree$cptable

# library(rpart.plot))
# rpart.plot(tree)


```
![Output tree plot of after setting cp=0.005 in rpart.control() function](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling/rcontrolcp.svg)

We can see for the output and the tree plot, that the tree size has increased, with increase in number of splits, and leaf nodes. Also we can see that the minimum CP value in the output is 0.005.


Now we saw the most important control parameters of the rpart.control function. Remember there are other parameters too, which you can study if you wish, but studying just these 3 discussed above are sufficient for the scope of this course.

**Also**, note we have not check the effects of the control parameters on the prediction accuracy of the decision tree. Using the control parameters you could either increase the accuracy, but also risk, decreasing the accuracy. So choosing the controls parameter very carefully is very important to push the accuracy in the right direction.

---

## Prediction using rpart. {#rpartpredict}

Now that we saw process to create a decision tree and also plotting it, we will like to use the output tree to predict the required attribute.

From the moody example, we are trying to predict the grade of students. Lets look at the `predict()` function to predict the outcomes.

- `predict(*object*,*data*,*type*,...)`
  - *object*: the generated tree from the rpart function.
  - *data*: the data on which the prediction is to be performed.
  - *type*: the type of prediction required. One of "vector", "prob", "class" or "matrix".

Now lets use the predict function to predict the grades of students using the tree generated on the Moody dataset.

```{r}
# First lets import the rpart library
library(rpart)

# Import dataset
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv")

# Use of the rpart() function.
tree <- rpart(GRADE ~ SCORE+ON_SMARTPHONE+ASKS_QUESTIONS+LEAVES_EARLY+LATE_IN_CLASS, data = moody[,-c(1)],method = "class")

# Now lets predict the Grades of the Moody Dataset.
pred <- predict(tree, moody, type="class")
head(pred)
```
We see that the output of the predict function is a vector of grades corresponding to each record of the Moody dataframe. Each index has a grade among "A", "B", "C", "D", "F". 

Although we see the output, how do we compare the accuracy and correctness of the outputs.

Lets look at one of the basic test we can do is perform a record by record comparison of the grade already in the dataset and the predicted grade, in the example below.

```{r}
# First lets import the rpart library
library(rpart)

# Import dataset
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv")

# Use of the rpart() function.
tree <- rpart(GRADE ~ SCORE+ON_SMARTPHONE+ASKS_QUESTIONS+LEAVES_EARLY+LATE_IN_CLASS, data = moody[,-c(1)],method = "class")

# Now lets predict the Grades of the Moody Dataset.
pred <- predict(tree, moody, type="class")
head(pred)

# Lets check the correctness of each prediction for each record
mean(moody$GRADE==pred)*100
```
Using just a row by row comparison we can see that the outcomes of the predicted grades and the original grades from the Moody dataset, are matching 93.73%. Thus our prediction accuracy is 93.73% and the error rate is 6.27%, which is very good.

This prediction accuracy calculated on the training dataset is called *training accuracy*.

Notice that we just compared the predicted grades with the already present grades from the Moody dataset, the same dataset on which the tree was built. 

In such scenario, one can say that you are predicting the grades on data already considered while training. So in some sense, you just output the known grades, and did not do any useful prediction.

But to really see the use of our tree, we must predict on a data which has never been used in the training of the tree, OR, where the Prof. Moody has not assigned the grades.

In the latter case it is difficult to prove the accuracy, without actually checking with Prof. Moody, if he/she would have assigned the same grade as the grade predicted by our model.

But we have a solution for the former case, where we can predict grades on a subset of data, which we have not used while training. For that we would need to split the provided data into 2 parts, Training and Testing and then repeat the training process on the training dataset and the prediction on testing dataset.

---

## Split the data yourself. {#splitdata}

We introduced the first and basic model for data analysis, Decision tree, in the section above. 

But we found that we need to perform a basic routine before dwelling deep into the creation of decision tree. 

The routine is to split the dataset in multiple parts, to check the accuracy of our tree's prediction. 

This routine is the first step you perform after acquiring a cleaned dataset.

The most useful splitting of dataset is done in 2 parts, Training and Testing.

- While splitting, the split ratio between training and testing should be decided properly. Mostly, training data is kept bigger,and testing is done on a relatively smaller subset. But the ratio should not be too biased, where there are only few observations in test data compared to training data. 
- Usually, the math behind splitting is that, even after split, the smaller subset, i.e. the test subset should represent the distribution of the complete dataset. This means the test data should at-least have few record of each possible combination of attribute's categories if categorical data, or, if numerical data then the numerical distribution is same as of the complete dataset.
- Thus, typically the train-test ratio is 80-20 or 70-30 or in some case even 60-40.

Also, while selecting the records to assign to either training/testing data, they should be randomly picked from the original data, so as to avoid unbalanced distribution.

We will look at a small example of splitting the complete dataset into training and testing dataset with a 70-30 ratio. 


```{r}
# Import dataset
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv")

# Split randomly into 2 sets with certain ratio/probability.
idx <- sample( 1:2, size = nrow(moody), replace = TRUE, prob = c(.7, .3))
moody.train <- moody[idx == 1,]
moody.test <- moody[idx == 2,]

nrow(moody)
nrow(moody.train)
nrow(moody.train)/nrow(moody)
nrow(moody.test)
nrow(moody.test)/nrow(moody)
```
As we can see we split the original data with 1580 rows into two dataset, training data with almost 70% of rows of the original, and testing data with almost 30% of the original. Notice that we used a random sampling of the data, and not just sequential, to avoid any unbalanced distribution of attributes.


Now, we looked at a method to split the dataset into training and testing data. But there is another type of splitting of the dataset which involves splitting the data into 3 parts namely, training, cross-validation and testing. We will look at the use of cross-validation and the process, in the next section \@ref(crossvalidation).

Typically, the ratio of train-validation-test is 60-20-20 or 50-25-25.

Before that lets look at a simple method to perform a 3 way split with ratio 60-20-20.

```{r}
# Import dataset
moody <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv")

# Split randomly into 3 sets with certain ratio/probability.
idx <- sample( 1:3, size = nrow(moody), replace = TRUE, prob = c(0.6, 0.2, 0.2))
moody.train <- moody[idx == 1,]
moody.validation <- moody[idx == 2,]
moody.test <- moody[idx == 3,]

nrow(moody)
nrow(moody.train)
nrow(moody.train)/nrow(moody)
nrow(moody.validation)
nrow(moody.validation)/nrow(moody)
nrow(moody.test)
nrow(moody.test)/nrow(moody)
```
We can see that the dataset is split into 3 parts, with 60% in training data, 20% in validation data, and 20% in testing data.

---

## Cross Validation {#crossvalidation}

Cross validation is a model validation technique for assessing generalization of the results of statistical analysis to an independent dataset. In other words, it is a technique to estimate the accuracy of a predictive model's performance in practice. 

The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating/training it, in order to avoid problems like *over-fitting* and *selection bias*, and to give an insight on how the model will generalize to an independent dataset(i.e., an unknown dataset).

Cross-validation also helps in selecting and fine-tuning the hyper-parameters of the models. In our case of decision tree, the hyper parameters could be the control parameters that determind the size of the decision tree, which in-turn determines the accuracy of the tree.

One round of cross-validation involves partitioning data into complementary subsets, and then performing model training on one subset, and validating the results on the other subset. In most methods, multiple rounds of cross-validation are performed using different partitions in each round, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.

Another use of cross-validation is when you don't have the test data, and hence, you don't have a way to determine the true accuracy of the model. Because we cannot determine accuracy on test dataset, we partition our training dataset into train and validation (testing). We train our model (rpart or lm) on train partition and test on the validation partition. 
The accuracy on the validation data is called *cross-validation accuracy*, while that on the train data is called *training accuracy*.

Lets not dive too deep into the theory of this cross validation technique, but lets learn about the cross_validate() function, that helps us achieve this.

- `cross_validate(*data*, *tree*, *n_iter*, *split_ratio*, *method*)`
  - *data*: The dataset on which cross validation is to be performed.
  - *tree*: The decision tree generated using rpart.
  - *n_iter*: Number of iterations.
  - *split_ratio*: The splitting ratio of the data into train data and validation data.
  - *method*: Method of the prediction. "class" for classification.

The way the function works is as follows:

- It randomly partitions your data into training and validation. 
- It then constructs the following two decision trees on training partition:
  -  The tree that you pass to the function.
  -  The tree constructed on all attributes as predictors and with no control parameters.
-It then determines the accuracy of the two trees on validation partition and returns you the accuracy values for both the trees. 

The first column corresponds to the cross-validation accuracy on the tree that you pass; the second is the cross-validation accuracy on the tree without any control and all attributes.

The values in the first column(accuracy_subset) returned by cross-validation function are more important when it comes to detecting overfitting. If these values are much lower than the training accuracy you get, that means you are overfitting.

We would also want the values in accuracy_subset to be close to each other (in other words, have low variance). If the values are quite different from each other, that means your model (or tree) has a high variance which is not desired.

The second column(accuracy_all) tells you what happens if you construct a tree based on all attributes. If these values are larger than accuracy_subset, that means you are probably leaving out attributes from your tree that are relevant.

Each iteration of cross-validation creates a different random partition of train and validation, and so you have possibly different accuracy values for every iteration.

Lets look at the cross_validate() function in action in the example below.

We will pass the tree with formula as `GRADE ~ SCORE+ON_SMARTPHONE+LEAVES_EARLY`, and control parameter, with `minsplit=100`. 
And for cross_validate() function, we will use` n_iter=5, and split_raitio=0.7` 

*NOTE*: Cross-Validation repository is already preloaded for the following interactive code block. Thus you can directly use the cross_validate() function in the following interactive code block. But if you wish to use the code_validate() function locally, please use 
```
install.packages("devtools") 
devtools::install_github("devanshagr/CrossValidation")
CrossValidation::cross_validate()
```


```{r,ex="crossvalidate",type="pre-exercise-code"}
library("rpart")

cross_validate <- function(df, tree, n_iter, split_ratio, method = 'class')
{
  # training data frame df
  df <- as.data.frame(df)

  # mean_subset is a vector of accuracy values generated from the specified features in the tree object
  mean_subset <- c()

  # mean_all is a vector of accuracy values generated from all the available features in the data frame
  mean_all <- c()

  # control parameters for the decision tree
  contro = tree$control

  # the following snippet will create relations to generate decision trees
  # relation_all will create a decision tree with all the features
  # relation_subset will create a decision tree with only user-specified features in tree
  dep <- all.vars(terms(tree))[1]
  indep <- list()
  relation_all = as.formula(paste(dep, '.', sep = "~"))
  i <- 1
  while (i < length(all.vars(terms(tree)))) {
    indep[[i]] <- all.vars(terms(tree))[i + 1]
    i <- i + 1
  }
  b <- paste(indep, collapse = "+")
  relation_subset <- as.formula(paste(dep, b, sep = "~"))

  # creating train and test samples with the given split ratio
  # performing cross-validation n_iter times
  for (i in 1:n_iter) {
    sample <-
      sample.int(n = nrow(df),
                 size = floor(split_ratio * nrow(df)),
                 replace = F)
    train <- df[sample,]
    testing  <- df[-sample,]
    type = typeof(unlist(testing[dep]))

    # decision tree for regression if the method specified is "anova"
    if (method == 'anova') {
      first.tree <-
        rpart(
          relation_subset,
          data = train,
          control = contro,
          method = 'anova'
        )
      second.tree <- rpart(relation_all, data = train, method = 'anova')
      pred1.tree <- predict(first.tree, newdata = testing)
      pred2.tree <- predict(second.tree, newdata = testing)
      mean1 <- mean((as.numeric(pred1.tree) - testing[, dep]) ^ 2)
      mean2 <- mean((as.numeric(pred2.tree) - testing[, dep]) ^ 2)
      mean_subset <- c(mean_subset, mean1)
      mean_all <- c(mean_all, mean2)
    }

    # decision tree for classification
    # if the method specified is not "anova", then this block is executed
    # if the method is not specified by the user, the default option is to perform classification
    else{
      first.tree <-
        rpart(
          relation_subset,
          data = train,
          control = contro,
          method = 'class'
        )
      second.tree <- rpart(relation_all, data = train, method = 'class')
      pred1.tree <- predict(first.tree, newdata = testing, type = 'class')
      pred2.tree <-
        predict(second.tree, newdata = testing, type = 'class')
      mean1 <-
        mean(as.character(pred1.tree) == as.character(testing[, dep]))
      mean2 <-
        mean(as.character(pred2.tree) == as.character(testing[, dep]))
      mean_subset <- c(mean_subset, mean1)
      mean_all <- c(mean_all, mean2)
    }
  }

  # average_accuracy_subset is the average accuracy of n_iter iterations of cross-validation with user-specified features
  # average_acuracy_all is the average accuracy of n_iter iterations of cross-validation with all the available features
  # variance_accuracy_subset is the variance of accuracy of n_iter iterations of cross-validation with user-specified features
  # variance_accuracy_all is the variance of accuracy of n_iter iterations of cross-validation with all the available features
  cross_validation_stats <-
    list(
      "average_accuracy_subset" = mean(mean_subset, na.rm = T),
      "average_accuracy_all" = mean(mean_all, na.rm = T),
      "variance_accuracy_subset" = var(mean_subset, na.rm = T),
      "variance_accuracy_all" = var(mean_all, na.rm = T)
    )

  # creating a data frame of accuracy_subset and accuracy_all
  # accuracy_subset contains n_iter accuracy values on cross-validation with user-specified features
  # accuracy_all contains n_iter accuracy values on cross-validation with all the available features
  cross_validation_df <-
    data.frame(accuracy_subset = mean_subset, accuracy_all = mean_all)
  return(list(cross_validation_df, cross_validation_stats))
}
```

```{r,ex="crossvalidate",type="sample-code",height=700}
# First lets import the rpart library
library(rpart)

# Import dataset
moody.train <- read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv",stringsAsFactors = T)

# Use of the rpart() function.
tree <- rpart(GRADE ~ SCORE+ON_SMARTPHONE+LEAVES_EARLY, data = moody.train[,-c(1)],method = "class",control = rpart.control(minsplit = 100))

# Now lets predict the Grades of the Moody Dataset.
pred <- predict(tree, moody.train, type="class")
head(pred)

# Lets check the Training Accuracy
mean(moody.train$GRADE==pred)

# Lets us the cross_validate() function.
cross_validate(moody.train,tree,5,0.7)

```

*NOTE*: If you encounter error while running the cross-validation function that said "new levels encountered" in test, make sure the dataset is imported again with read.csv() attribute `stringsAsFactors` as `TRUE or T`. For more information about the inner-working of the cross_validate() function visit [cross_validate()](https://github.com/devanshagr/CrossValidation/blob/master/R/cross_validation.R)

We can see in the output the Training accuracy, the table of cross-validation accuracy at each iteration for both the passed tree and the tree on all attribute and also their averages and variances.

Few Observation from the selected example above are:

- For the tree passed with selected attributes and some control parameters, the cross-validation accuracy's (i.e. accuracy values in the `accuracy_subset` column) are fairly high for all iterations and have very low variance.
- They are close to the training accuracy which indicates we are not overfitting.
- Observe that the accuracy at each iteration of the accuracy_subset and accuracy_all column are relatively, close but not exact, suggesting that there are more attributes or other control parameters that can be included to the passed tree, to further increase the accuracy, thus closing the gap.

Thus using cross-validation we were able to figure out with certainty, that the passed tree, is not the best tree that can be created using the training data. Also, we saw whether the generated tree overfits the training data or not.

---
EOC

<!--chapter:end:chapters/modeling.Rmd-->

# Data Modelling and Prediction techniques for Regression. {#regression}

<script src="files/js/dcl.js"></script>
```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```


In chapter \@ref(classification) we studied modeling and prediction techniques for Classification, where we predicted the class of the output variable, using Decision tree based algorithms.

In this chapter we will study techniques for regression based prediction and create models for it. As opposite the the classification where we predict a particular class of the output variable, here we will predict a numerical/continuous value.

We will look at two methods of performing this regression based modeling and prediction, first simple linear regression and second regression using decision tree.

---

## Linear Regression.

Linear regression is a linear approach to modeling the relationship between a *scalar* response ($Y$) and one or more *explanatory* variables($X_i$, where i is the number of explanatory variables).

- Scalar response means the predicted output. These variables are also known as dependent variables i.e. they are derived by applying some law/rule or function onto some other variable/s
  - Usually in linear regression, models are used to predict only one scalar variable. But there are two subtype if these models:
    - First when there is only one explanatory variable and one output variable. This type of linear regression model known as *simple linear regression*.
    - Second, when there are multiple predictors, i.e. explanatory/dependent variables for the output variable. This type of linear regression model known as *multiple linear regression*.
  - But in the case of prediction of multiple correlated output variables, we call this type of prediction using linear regression model as *multivariate linear regression*.
- Explanatory variables are the predictors on which the output predictions are based on. These variables are also known as independent variables and are independently sufficient to be used as predictors in regression models.

![Linear models fitted to various different type of data spread. This illustrates the pitfalls of relying solely on a fitted model to understand the relationship between variables. Credits: Wikipedia.](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling2/lmvariants.svg)

Since the relationship between the explanatory variables and the output variable is modeled linearly, these models are called as linear models. To do this, we need to find a linear regression equation for the set of input predictors and the output variable. 

But without going into the mathematics of finding this linear regression equation, we will use a tool/function provided in R to model and predict the output variable.

---

### Linear regression using lm() function {#lm}

Syntax for building the regression model using the *lm()* function is as follows:

- `lm(formula, data, ...)`
  - *formula*: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. 
    - `prediction ~ predictor1 + predictor2 + predictor3 + ...`
  - *data*: here we provide the dataset on which the linear regression model is to be trained.
  
For more info on the *lm()* function visit [lm()](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm)

Lets look at the example on the RealEstate dataset.

A snippet of the Realestate Dataset is given below.

```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/RealEstate.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Real Estate Dataset',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

Now we can build a simple linear regression model to predict the Price attribute based on the various other attributes present in the dataset, as shown above. 

Since we will be predicting only one attribute values, this model will be called simple linear regression model.

For the first example we will predict the price value of house using only  *size* attribute as the predictor.

```{r}
library(ModelMetrics)

# Load the dataset.
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/RealEstate.csv")

# spliting the dataset into training and testing.
idx <- sample( 1:2, size = nrow(realestate), replace = TRUE, prob = c(.7, .3))
train <- realestate[idx == 1,]
test <- realestate[idx == 2,]

# Use the lm() function to predict the price based on size of the house.
# Thus this is an example of simple linear regression since only one predictor and one output value is used.
simple.fit <- lm(Price~Size,data=train)

# summary of the model
summary(simple.fit)

# Linear relation between the Price and Size attribute.
plot(train$Size,train$Price)
abline(simple.fit , col="red")

# Predicting values on the test dataset.
PredictedPrice.simple <- predict(simple.fit,test)
# Predicted Values
head(as.integer(unname(PredictedPrice.simple)))
# Actual Values
head(test$Price)
```
We can see that,

- The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc.
- The plot of *Size* vs *Price*, and the red line represents the fitted line or the linear model line which will be used for prediction.
- The predicted values are obtained form the predict function using the trained model and the test data. In comparison to the actual values, the predicted values are some times close,some time far and few are very far. 


We saw above an example of simple linear regression model, where only one predictor was used for predicting a single output attribute.

Now we will see an example of *multiple linear regression* model, where there can be multiple predictors to predict a single output attribute. (Note: Please do not confuse this with the *multivariate linear regression*.)

Let look at an example of predicting the *Price* of the real estate, based on 3 attributes *Size, Number of Bedrooms and Number of Bathrooms*.

```{r}
library(ModelMetrics)

# Load the dataset.
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/RealEstate.csv")

# spliting the dataset into training and testing.
idx <- sample( 1:2, size = nrow(realestate), replace = TRUE, prob = c(.7, .3))
train <- realestate[idx == 1,]
test <- realestate[idx == 2,]


# Use the lm() function to predict the price based on size, bathrooms and bedrooms of the house.
# Thus this is an example of multiple linear regression since multiple predictor and one output value is used.
multiple.fit <- lm(Price~Size + Bathrooms + Bedrooms,data=train)

# summary of the model
summary(multiple.fit)

# Predicting values on the test dataset.
PredictedPrice.multiple <- predict(multiple.fit,test)
# Predicted Values
head(as.integer(unname(PredictedPrice.multiple)))
# Actual Values
head(test$Price)
```
We can see that,

- The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc.
- The predicted values are obtained form the predict function using the trained model and the test data. In comparison to the previous model based on just the *Size* as predictor, here, when we used 3 predictors, we have more accurate predictions, thus increasing the overall accuracy of the model.

---

### Calculating the Error using mse() {#mse}

As was the simple case in the categorical predictions of the classification models, where we could just compare the predicted categories and the actual categories, this type of direct comparison as an accuracy test won't prove useful now in our numerical predictions scenario.

Also we don't want to eyeball everytime we predict, to find the accuracy of our predictions each row by row, so lets see a method to calculate the accuracy of our predictions, using some statistical technique.

To do this we will use the Mean Squared Error(MSE).

- The MSE is a measure of the quality of an predictor/estimator
- It is always non-negative
- Values closer to zero are better.

The equation to calculate the MSE is as follows:

\begin{equation}
MSE=\frac{1}{n} \sum_{i=1}^{n}{(Y_i - \hat{Y_i})^2}
\\ \text{where $n$ is the number of data points, $Y_i$ are the observed value}\\ \text{and $\hat{Y_i}$ are the predicted values}
\end{equation}

To implement this, we will use the *mse()* function present in the Metrics Package, so remember to install the Metrics package and use `library(Metrics)` in the code for local use.

The syntax for *mse()* function is very simple:

- `mse(actual,predicted)`
  - *actual*: vector of the actual values of the attribute we want to predict.
  - *predicted*: vector of the predicted values obtained using our model.

Now lets look at the MSE of the previous example.

```{r}
library(ModelMetrics)

# Load the dataset.
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/RealEstate.csv")

# spliting the dataset into training and testing.
idx <- sample( 1:2, size = nrow(realestate), replace = TRUE, prob = c(.7, .3))
train <- realestate[idx == 1,]
test <- realestate[idx == 2,]

# Use the lm() function to predict the price based on size of the house.
simple.fit <- lm(Price~Size,data=train)
# Predicting values on the test dataset.
PredictedPrice.simple <- predict(simple.fit,test)
# Predicted Values
head(as.integer(unname(PredictedPrice.simple)))
# Actual Values
head(test$Price)

# Lets use the mse() function to 
mse(test$Price,PredictedPrice.simple)

```
We can see the MSE is too large above 200 billion, and this is huge value could be understandable as we are taking the squared differences of all the records that we predicted. 

The main intention is to get this huge value to as low as possible possibly near zero, which could be difficult but can be achieved upto a relative error by using a better model and training data.

---

## Regression using RPART

Since we have already used the rpart library for performing decision tree algorithms also referred as CART(classification and regression tree) algorithms, we will now look at this type algorithm for regression based prediction.

Remember we have discussed the usage of Rpart in the section \@ref(rpart) in great detail. Thus for using Rpart for regression based prediction we will need to provide the *rpart()* functions, *method* attribute, with the keyword *"anova"*. 

For more details on the use of Rpart for prediction please refer to section \@ref(rpart).

Lets look at an example of regression based prediction using Rpart for the *Price* attribute of the Real estate Dataset with *Size, Number of Bedrooms and Number of Bathrooms* as predictors.

```{r}
library(rpart)
library(ModelMetrics)

# Load the dataset.
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/RealEstate.csv")

# spliting the dataset into training and testing.
idx <- sample( 1:2, size = nrow(realestate), replace = TRUE, prob = c(.7, .3))
train <- realestate[idx == 1,]
test <- realestate[idx == 2,]

# Use of the rpart() function to predict the price based on the size, bathrooms and bedrooms of the house.
rpart.fit <- rpart(Price~Size+Bathrooms+Bedrooms,data=train,method = "anova")

# Predicting values on the test dataset.
PredictedPrice.rpart <- predict(rpart.fit,test)
# Predicted Values
head(as.integer(unname(PredictedPrice.rpart)))
# Actual Values
head(test$Price)

# Lets use the mse() function to 
mse(test$Price,PredictedPrice.rpart)
```

![Output tree plot of rpart() model using for regression using "anova" method](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling2/rpartanova.svg)

We can see,

- The output decision tree of the *rpart()* function
- The predicted values obtained using the model created by the rpart() function.
- The MSE of the model on the testing dataset.

An important point to note while using decision trees for regression purpose, is that since the underlying process of modeling is still a decision tree, the output still represent a set of distinct classes, even though the values of the classes are numeric. Thus we can see that the predicted values are repeated even for varying inputs.

Hence Decision tree must be used carefully when used for regression based prediction models.

---
EOC

# Additional Modeling techniques. {#models}

<script src="files/js/dcl.js"></script>


In this chapter, we will see some additional machine learning models used in practice, for various purpose.

After studying both classification models and regression models in the previous 2 chapters \@ref(classification) & \@ref(regression) respectively,  we will now look into other generic models used for classification and/or regression purpose.

Below is the list some of the widely used algorithms with their use case(either classification/regression or both) and training and prediction complexities for using particular learning models.

![Usage and Complexity of various machine learning algorithms. Credits:thekerneltrip.com](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling2/MLmodelcomplexity.png)

As we can see that many of these algorithms can be used for classification and regression all together, as we saw in the case of the Decision tree models using Rpart in section \@ref(decisiontree), and also some model used for only a particular type of prediction e.g. linear regression.

We will look at few algorithms from the above list:

- Random Forest
- Support Vector Machine (SVM)
- Neural Metworks
- and also Linear Discriminant Analysis (LDA)

---

## Four Line Method for creating most type of prediction models in R {#model4step}
But before we learn about these algorithms, let us see a four line method to build models using any of the above algorithms using R.

We can safely assume that the data going to be used to build the model, has been pre-processed and based on requirements split into the required subsets. To see how to split the data refer to section \@ref(splitdata).

0. The zeroth step now will be obviously to install and load the packages that contain the ML algorithm. To do that on your local machine, use the following code.
``` text
# Install the library
install.packages("package name")

# Load the library in R
library(package_name)
```
1. Once we have the algorithm library loaded, we then proceed to build the model.
- `pred.model = model_function(formula, data, method*,...)`
  - *model_function()*: the function present in the library to build the model. e.g. *rpart()* 
  - *formula*: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. 
    - `prediction ~ predictor1 + predictor2 + predictor3 + ...`
  - *data*: here we provide the dataset on which the ML model is to be trained on. Remember never used the test data to build the model.
  - *method*: (OPTIONAL) Used to denote the method of prediction or underlying algorithm. This parameter could be present in some model_function() but not all.
2. Prediction using the predict() function on the training data to assess the models performance/accuracy in next step.
- `pred = predict(pred.model, newdata = train)`
  - *predict()*: the common function for all models used for prediction.
  - *pred.model*: output of the step 1.
  - *newdata*: here we assign the data on which the prediction is to be done.
3. Evaluate error in Training phase. We use the mse() function for finding the accuracy of the model. To read more in dept about the mse() function refer to section \@ref(mse).
- `mse(actual, pred)`
  - *actual*: vector of the actual values of the attribute we want to predict.
  - *pred*: vector of the predicted values obtained using our model.

Repeat steps 0,1,2 and 3 by changing the ML algorithm or manipulating dataset to perform better when used to train using ML model, so as to achieve as low MSE value as possible.

4. Finally we predict on the testing data using the same predict function as in step 2 but replacing the train data with test data.
- `pred = predict(pred.model, newdata = test)`

These are the 4 steps to follow while performing any prediction task using ML models in R.

We can also add one more step between step 3 and 4, which is step of performing the cross validation process on the newly built models. 

This can be done either manually, or by using third party libraries. 

One such library is the `rModeling` package, which has function *crossValidation()* which can be used for any type of model_functions(). For more information visit [crossValidation()](https://www.rdocumentation.org/packages/rModeling/versions/0.0.3/topics/crossValidation)


Before we proceed to the next section, please look at the snippet of the *earnings.csv* dataset, which we will be using for predicting the *Earnings* attribute based on various other attributes provided in the dataset, using different prediction models.

```{r,echo=FALSE,error=FALSE,warning=FALSE}
earningdata<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/earnings.csv") #web load
temp<-knitr::kable(earningdata[sample(nrow(earningdata),10),], caption = 'Snippet of Earnings Dataset',booktabs=TRUE) 
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

Now that we saw the general structure of the model and took a glace at the dataset we will be using, lets look at few of the algorithms as we promised from the list above.

---

## Random Forest {#randomforest}

Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the value that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees. 

In simpler terms, the random forest  algorithm creates multiple decision trees based on varying attributes and biases, and then predicts the output for each tree, and aggregates this prediction into one final output by some technique like majority count or average/etc.

![Visual Representation of a Random Forest learning model. Credits: Random Forest Wikipedia](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling2/randomforest2.png)

The main idea behind Random Forest arise from a method called ensemble learning method.

Ensemble learning is the method of solving a problem by building multiple ML models and combining them. It is primarily used to improve the performance of classification, prediction, and function approximation models.

Forests are type of ensemble learning methods, where they act like, pulling together all of decision tree algorithm efforts. Taking the teamwork of many trees thus improving the performance of a single random decision tree.

Random decision forests correct for decision trees' habit of overfitting to their training set.

Now lets look at an example of prediction by the random forest model using the *randomForest()* function present in the `randomForest` library package. For more information about the *randomForest()* function and its attributes visit **[randomforest()](https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest)**

Thus following the 4 step method of prediction for predicting a numerical attribute "Earnings" using the randomForest() function.

```{r}
library(randomForest)
library(ModelMetrics)

# Load the dataset.
earningdata<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/earnings.csv",stringsAsFactors = T)

# splitting the dataset into training and testing.
idx <- sample( 1:2, size = nrow(earningdata), replace = TRUE, prob = c(.8, .2))
train <- earningdata[idx == 1,]
test <- earningdata[idx == 2,]

# 1. Build prediction model using randomForest() function.
pred.model <- randomForest(Earnings ~., data = train)

# Lets see the summary of the randomForest model.
pred.model

# 2. Predict using the newly built model on the training dataset.
pred.train <- predict(pred.model,newdata = train)

# 3. Evaluate error on training using the mse() function.
mse(train$Earnings,pred.train)

# 4. Predict on the testing data.
pred.test <- predict(pred.model,newdata = test)

# Additionally since here we have the actual/real prediction values we can also check the accuracy of our prediction on testing data.
mse(test$Earnings,pred.test)

```
We can see,

- the summary of the output randomForest model with details of:
  - Formula used.
  - Type of random forest
  - Number of trees created in the forest
  - Number of variables used at each split.
  - And some performance parameter.
- The mean squared error of the predicted values using training sub-dataset.
- The mean squared error of the predicted values using the testing sub-dataset.

Note: Since a random forest is an ensemble learning method, it will usually take a lot more time to train that its counterparts. Thus you can see a significant waiting/execution time while running the above code and acquiring answer.

---

## SVM {#svm}

Support-Vector Machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification(mostly) and regression(also works in some cases) analysis.

The goal of the SVM is to find a hyperplane in an N-dimensional space (where N corresponds with the number of features) that distinctly classifies/regresses the data points. The accuracy of the results directly correlates with the hyperplane that we choose. We should find a plane that has the maximum distance between data points of both classes.

Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier.

![Support Vector Machine Linear classification hyperplane(line) example. Credits: Support Vector Machines Wikipedia](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling2/svm2.png)

Note that the dimension of the hyperplane depends on the number of features. If the number of input features is two, then the hyperplane is just a line. If the number of input features is three, then the hyperplane becomes a two-dimensional plane. It becomes difficult to draw on a graph a model when the number of features exceeds three.

In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.

![Example of the kernal trick for non-linear classifier. Credits: Support Vector Machines Wikipedia](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling2/Kernel_Machine.svg)

Why is this called a support vector machine? *Support vectors* are *data points* closest to the hyperplane. They directly influence the position and orientation of the hyperplane and minimizes the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These points thus help to build our SVM model.

SVM is great because it gives quite accurate results with minimum computation power.

Lets look at the example of Support vector machine algorithm in use for predicting the *Earnings* attribute of the Earnings dataset. 

We will use the *svm()* function from the *e1071* package. For more information about this function and its attributes visit **[svm()](https://www.rdocumentation.org/packages/e1071/versions/1.7-6/topics/svm)**


Thus following the 4 step model for prediction and using the "*svm()*" function as the model function. 

```{r}
library(e1071)
library(ModelMetrics)

# Load the dataset.
earningdata<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/earnings.csv",stringsAsFactors = T)


# splitting the dataset into training and testing.
idx <- sample( 1:2, size = nrow(earningdata), replace = TRUE, prob = c(.8, .2))
train <- earningdata[idx == 1,]
test <- earningdata[idx == 2,]

# 1. Build prediction model using svm() function.
pred.model <- svm(Earnings ~., data = train)

# Lets see the summary of the svm model.
pred.model

# 2. Predict using the newly built model on the training dataset.
pred.train <- predict(pred.model,newdata = train)

# 3. Evaluate error on training using the mse() function.
mse(train$Earnings,pred.train)

# 4. Predict on the testing data.
pred.test <- predict(pred.model,newdata = test)

# Additionally since here we have the actual/real prediction values we can also check the accuracy of our prediction on testing data.
mse(test$Earnings,pred.test)

```
We can see,

- the summary of the output svm model with details of:
  - Formula used.
  - Type of SVM model
  - The SVM Kernel Used
  - And some performance parameter.
  - Also, the Number of Support Vectors.
- The mean squared error of the predicted values using training sub-dataset.
- The mean squared error of the predicted values using the testing sub-dataset.


---

## Neural Network. {#nnet}

An Artificial Neural Network (ANN), usually simply called neural network(NN) is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. 

Thus in other words, a neural network is a sequence of neurons connected by synapses, which reminds of the structure of the human brain. However, the human brain is even more complex, and a NN is just a model that mimics a human brain.

An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. 

Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. 

Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. 

Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.

![A visual representation of typical neural network with various nodes and edges along with layers. Credits: Artificial Neural Network Wikipedia](https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/img/modeling2/neuralnetwork.svg)

What is great about neural networks is that they can be used for basically any task from spam filtering to computer vision. However, they are normally applied for machine translation, anomaly detection and risk management, speech recognition and language generation, face recognition, and more.

To accommodate such a wide variety of application, neural nets are transformed and models in various different ways. To find multiple types of neural networks please visit **[Neural Network Zoo](https://www.asimovinstitute.org/neural-network-zoo/) **

Now lets try to implement a neural network learning model for the Earnings prediction problem of Earnings dataset. 

To do this we will use the 4 step method of prediction and use the *nnet()* function from the "nnet" package as the model_function.

Let look at the *nnet()* function and its parameters.

- `nnet(formula,data,size,linout,...)`
  - *formula* and *data* are the same as mentions in Step 1 of section \@ref(model4step).
  - *size*: denotes the number of units in the hidden layer.
  - *linout*: Assign TRUE is predicting numerical value. Default is FALSE, for predicting categorical value.
  - *rang*: set the initial random weights on each edge.
  - *maxit*: maximum number of iterations.
  - *decay*: weight decay parameter. Can also be understood as learning rate.
- **Note**: The *nnet()* function can only create a single hidden layer neural network model. To create more complex models please use different packages like [neuralnet](https://www.rdocumentation.org/packages/neuralnet/versions/1.44.2) or *h2o, deepnet*, etc

Now lets use the *nnet()* function for predction.

```{r}
library(nnet)
library(ModelMetrics)

# Load the dataset.
earningdata<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/earnings.csv",stringsAsFactors = T)

# splitting the dataset into training and testing.
idx <- sample( 1:2, size = nrow(earningdata), replace = TRUE, prob = c(.8, .2))
train <- earningdata[idx == 1,]
test <- earningdata[idx == 2,]

# 1. Build prediction model using nnet() function.
pred.model <- nnet(Earnings/20000 ~., data = train, size = 50, decay=5e-5,maxit = 500,linout = T)

# Lets see the summary of the nnet model.
pred.model

# 2. Predict using the newly built model on the training dataset.
pred.train <- predict(pred.model,newdata = train)*20000

# 3. Evaluate error on training using the mse() function.
mse(train$Earnings,pred.train)

# 4. Predict on the testing data.
pred.test <- predict(pred.model,newdata = test)*20000

# Additionally since here we have the actual/real prediction values we can also check the accuracy of our prediction on testing data.
mse(test$Earnings,pred.test)
```

**NOTE**: If you see a very high value of MSE after running the above code, please re-run it. Usually you will find the MSE to be better than all the models we have studied uptil now for the Earnings prediction problem.

We can see from the output,

- the summary of the output, neural network model, with details of:
  - Number of weights in the complete neural network
  - Initial Value and Final Value of the model weights along with iter value.
  - Structure of the neural network in I-H-O format where the numbers, I is input, H is hidden and O is output nodes.
  - Input node attributes. 
    - Note that the Majors column attribute are split into unique number of factors, thus creating new individual attributes.
  - Output node attributes.
  - Network Options.
- The mean squared error of the predicted values using training sub-dataset.
- The mean squared error of the predicted values using the testing sub-dataset.

---

After Comparing all these models, we can see that the MSE values for the 3 models are SVM > Random Forest > Neural Network. This suggest one trend that, to get as best result as possible, one must invest most time in choosing the right model,and use the model with cleaned dataset for training. 

Eventually, since we use R language here, the code for model creation just boils down to few lines of code, 4 steps to be more accurate. But since we might find one model works better than other, we must choose the best fit model.

Also, we saw the trend of time required for training of the models studied above was SVM > RandomForest > Neural Net. This also suggest a proportional relationship with the time required for a particular model to train and in turn producing the best possible results.

Although one can say that, we can just use Neural Networks all the time, well this statement is true to some extent, but depending on the resources, the complexity of the data, and the complexity of the model itself, one needs to make some trade-offs. One should not try to throw a ball just few meters with a cannon, using mere hands will do the job. In essence, do not try to overuse the neural net model for the sake of adding 2 mumbers, simple addition will suffice. Studying these tradeoffs and more models in depth though is out of this course scope.

---

**EOC**

<!--chapter:end:chapters/modeling2.Rmd-->

# Blog: Prediction Challenges {#predblogs}

<script src="files/js/dcl.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

```{r ,include=FALSE}
tutorial::go_interactive(greedy=TRUE)
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```


Until we have studied multiple methods of data analysis in sections \@ref(freestyle),\@ref(datatransformation), statistical testing in sections \@ref(stateval), &   building prediction models for both classification \@ref(classification) and regression \@ref(regression) along with advanced ML models \@ref(models).

Now its time to utilize them in various ways for  analysis and prediction of data.

To do this, in this course, we have designed few prediction challenges, which test your ability to implement skills learnt in the course until now. 

First challenge is a basic prediction challenge using only data analysis using the freestyle techniques from section \@ref(freestyle). 

Then onwards, prediction challenges used multitude of modeling techniques which were studied in \@ref(classification) and \@ref(regression).

---

## General Structure of the Prediction Challenges.

Usually there is a task to be performed in each prediction challenge. 

Either predicting a numerical of categorical values is the task of each challenge.

The way to perform those task are constrained differently for different prediction challenges based on levels of difficulty and ML models to be used.

The submission will take place on **Kaggle** which is used for organizing these prediction challenges online, helping in validating submissions, placing deadlines for submission and also calculating the prediction scores along with ranking all the submission.

The datasets provided for each prediction challenge is as follows:

1. Training Dataset.
    - It is used for training and cross-validation purpose in the prediction challenge. 
    - This data has all the training attributes along and the ideal values of the prediction attribute.
    - Models for prediction are to be trained using this dataset only.
2. Testing Dataset.
    - It is used for prediction only.
    - It consists of all the attributes that were used for training, but it does not contain any values of the actual prediction attributes, which is actually the attribute that the prediction challenge predicts.
    - Since its only used for prediction purpose and is not involved in training of the models, it is thus not involved in the cross-validation phase too.
3. Submission Dataset.
    - After prediction using the "testing" dataset, for submitting on Kaggle, we must copy the predicted attribute column to this Submission Dataset which only has 2 columns, first an index column(e.g. ID or name,etc) and second the predicted attribute column.
    - Remember after copying the predicted attribute column to this dataset, one should also save this dataset into the same submission dataset file, which then can be used to upload on Kaggle.

- To read the datasets use the *read.csv()* function and for writing the dataset to the file, use the *write.csv()* function.
  - Offen times while writing the dataframe from R to a csv file, people make mistake of writing even the row names, which results in error upon submission of this file to Kaggle.
  - To avoid this, you can add the parameter, `row.names = F` in the `write.csv()` function. e.g. `write.csv(*dataframe*,*fileaddress*,row.names = F)`.

Now lets look at the prediction challenges that took place in this course along with the top submissions by students.

---


## Prediction Challange 1.

In Prediction challange 1, the task was to predict a categorical value using *only free-style* prediction. 

For this prediction challenge we used our favorite dataset, the Moody dataset, and predicted the Grade category of all students. The Grade category had only 2 factors: *Pass* OR *Fail*.

Let look at a snippet of the moody dataset used for training in this challenge.

```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021train.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Moody Dataset(TRAINING) for Prediction Challenge 1',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")

```

We can see that there are multiple attributes like *Score, Attendence, Major, etc.* that can be used as predictors, and then there is *Grade* attribute with ideal values for each record of student which will be used while training and then will be predicted on the testing dataset.

Lets look at the snippet of the moody dataset for testing.

```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021test-students.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Moody Dataset(TESTING) for Prediction Challenge 1',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

We can see that the *Grade* attribute is not present in this dataset, since it is the attribute that will be predicted using our analysis of the training dataset.

Also, lets look at the submission file for prediction challenge 1.

```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/M2021test-submission-file.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Submission file for Prediction Challenge 1',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

We can see there are only 2 columns *Studentid* and *Grade*. *Studentid* column's entries corresponds/are similar to the *Studentid* column of the testing data. Thus we need to just fill the *Grade* column with appropriate grade predicted by our analysis, corresponding to the same *Studentid* values in both test and submission data.

Now that we have seen the data, feel free to go to the Kaggle site of this prediction challenge and take the challenge yourself. The link for challenge: [Prediction Challenge 1](https://www.kaggle.com/t/8099c3c8bd5940928d102a6ddda0ee3d){target="_blank"}

---

### Top Submissions for Challenge 1.

1. *Jeremy Prasad*     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred11">Jeremy's PPT</button>
<div id="pred11" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/jeremypred1.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - Jeremy performed exceptionally well in this prediction challenge.
  - His approach was a iterative learning process, where at each step after performing analysis he tried to decrease the error more and more.
  - He started with a very basic model, of using just the score attribute with a hard threshold for pass or fail grade based on the score value. 
  - After this, to increase accuracy, he analysed the data more found which attributes effect the prediction of the data, and which are not really useful
  - After finding these highly effective attributes, he wrote concrete set of attributs that can be used to assign the grade. Most of them were dependent on 2-3 attributes like Major-Senioriy-Score, Major-Score, or Major-Questions-Score,etc. 
  - This gave him a much better accuracy value for prediction. 


2. *Rohit  Manjunath*    <button class="btn btn-primary" data-toggle="collapse" data-target="#pred12">Rohit's PPT</button>
<div id="pred12" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/rohitpred1.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - Rohit performed well in this prediction challenge, and has a different approach than that of Jeremy's.
  - In Rohit's approach, instead of finding the minimum global threshold of pass or fail based on score, he found the threshold for the maximum score, above which every student passed the class.
  - He then analysed the data based on the Majors first and then found interval threshold for each Majors scores.
  - For some Majors, to increase accuracy, he further explored other attributes in detail to find which effects the final grade.
  - Rohit obtained accuracy of almost 85%.
  


---

## Prediction Challenge 2.

In Prediction Challenge 2, we introduced the use of Decision Tree algorithm for prediction model building, to complete the same task as we saw in the Prediction Challenge 1.

This was intended to see the first learning model in action, and also to see the ease in which the process of prediction can be completed using such prediction model against the trivial data analysis techniques.

The datasets for this prediction challenge were the same as those in the prediction challenge 1.

Since, the task in the prediction challenge was to predict a categorical value(*Grade* value) the learning algorithm allowed to be used in this task was the Decision Tree algorithm based on the CART model. Read more about how to use decision tree's in section \@ref(decisiontree) .

To implement this algorithm, students were allowed to use the RPART package \@ref(rpart)

With rpart() doing most work of prediction in this task, the students were also asked to provide validation for their models prediction power/accuracy. This involved use of cross-validation techinques, which for the ease of this course level was provided in a custom function, see \@ref(crossvalidation).

To perform this challenge yourself please visit the kaggle site of this prediction challenge. Link to Kaggle Site: [Prediction Challenge 2](https://www.kaggle.com/t/607a8221c6a647048f88ffa380ad1e4b){target="_blank"}

---

### Top Submissions for Challenge 2

1. Kevin Larkin    <button class="btn btn-primary" data-toggle="collapse" data-target="#pred21">Kevin's PPT</button>
<div id="pred21" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/kevinpred2.pdf&embedded=true" width="100%" height="500px"></embed>
</div>  
  
  - This was the top submission in terms of accuracy score on Kaggle.
  - Kevin used the rpart() function, for modeling, with all the attributes of the training dataset except *Studentid*.
  - To increase the accuracy of his model, he used the `rpart.control()` function parameters, especially the `cp` parameter of the function, which increased the splitting accuracy.
  - Kevin acheived an accuracy score of over 86% on the test dataset for this challenge.
  


2. Michael Ryvin    <button class="btn btn-primary" data-toggle="collapse" data-target="#pred22">Michael's PPT</button>
<div id="pred22" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/michaelpred2.pdf&embedded=true" width="100%" height="500px"></embed>
</div>
 
  - This was the second best submission as per accuracy score on Kaggle.
  - Michael used the rpart() function, along with some control parameters for creating the decision tree.
  - Michael acheived an accuracy score of over 86% on the test dataset.
 
  
  
3. Shuohao Ping    <button class="btn btn-primary" data-toggle="collapse" data-target="#pred23">Shuohao's PPT</button>
<div id="pred23" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/shuohaopred2.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was the third best submission as per accuracy score on Kaggle.
  - Shuohao used multiple iterations to create his final model.
  - In each iteration, Shuohao tried to vary the control parameters and its values to find the best fit model after cross-validation.
  - Shuohao, acheived an accuracy score of over 86% on the test dataset.
  



---

## Prediction Challenge 3.

After studying prediction of categorical data in the previous 2 prediction challenges, in prediction challenge 3, the task was to predict *Earnings* a numerical variable, using any ML algorithm. 

*Earnings* variable is part of the Earnings dataset which has details about a persons connections, GPA, Major,etc, and using these attributes, the students had to predict the numerical value of earnings of each person in the dataset.

Students were recommended to first find some correlation between data by using free-style analysis, and then proceed to using ML models. This was included so as to show the effect of human intervention/input on the selection and performance of ML model, and also to avoid the trap of blindly applying the most costly ML model which might perform well, but is a overkill to perform task which could be completed using other less costly models. ( Cost here refers to the computation resources and time involved in training the models. )

To read more about prediction of a numerical variable in R, see section \@ref(regression) and \@ref(models)

Lets look at a snippet of the Earnings dataset used for training the models below.

```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/Earnings_Train2021.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Earnings Dataset(TRAINING) for Prediction Challenge 3',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")

```

We can see that there are multiple attributes like *GPA,Major,Graduation_Year,Height,etc.* that can be used as predictors, and then there is *Earnings* attribute with ideal values for each record of student which will be used while training and then will be predicted on the testing dataset.

Lets look at the snippet of the earnings dataset for testing.

```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/Earnings_Test.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Earnings Dataset(TESTING) for Prediction Challenge 3',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

We can see that the *Earnings* attribute is not present in this dataset, since it is the attribute that will be predicted using our analysis of the training dataset.

Also, lets look at the submission file for prediction challenge 3.

```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/earning_submission.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Submission file for Prediction Challenge 3',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

We can see there are only 2 columns *ID* and *Earnings*. *ID* column's entries corresponds/are similar to the *ID* column of the testing data. Thus we need to just fill the *Earnings* column with appropriate earning value predicted by our analysis.

Now that we have seen the data, feel free to go to the Kaggle site of this prediction challenge and take the challenge yourself. The link for challenge: [Prediction Challenge 3](https://www.kaggle.com/t/951a9ad1d7e9444bb29b0dca65aed1cd){target="_blank"}


---


### Top Submissions for Challenge 3

1. Seok Yim     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred31">Seok's PPT</button>
<div id="pred31" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/seokpred3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was the top submission based on MSE score, with a final score less than 100.
  - The approach to solving this challenge was really well implemented. 
    - First, he looked at the dataset on whole, tried to find some interesting patterns.
    - Then, after finding the patterns, he did not predict on the complete dataset using one big model, but subseted the data based on one attribute, and then modeled the ML model on these small subsets.
  - This not only reduced the MSE to such low levels, thus increasing accuracy, but also led to faster model learning time, and prediction time.
  
  
2. Nick Whelan     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred32">Nick's PPT</button>
<div id="pred32" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/nickpred3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was another top submission based on MSE score, with final score less than 100.
  - The approach to solving the task was different compared to Seok's implementation, but was equally good, with nearly the same prediction power/accuracy.
    - Nick tried to use the randomForest algorithm on the whole dataset as the initial model, but the MSE turned out to be near 25,000.
    - Then he did some free-style analysis and found the linear relationship between various subsets of dataset with the *earnings* value.
    - To implement this he used the fundamentals of linear regression very well while creating a learning model, and also used a quadratic model where needed.
  - This resulted in a very accurate model with low MSE score.


3. Bennett Garcia     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred33">Bennett's PPT</button>
<div id="pred33" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/bennetpred3.pdf&embedded=true" width="100%" height="500px"></embed>
</div>
    
  - Bennett had a final MSE score of below 100 and was one of the top submissions for this challenge.
  - A significantly different learning model was used by Bennett to achieve this low MSE.
    - He first analyzed the data, and found attributes on which the dataset can be subsetted on.
    - Then, he here used Neural Networks as models for prediction on those subsets.
    - This Neural Network approach was very well implemented.
    
---

    
## Prediction Challenge 4.

Challenge 4 was a relatively newer challenge, and was built to test and combine all that has been learnt from the previous challenges.

In this challenge, there was a scenario as described below:

*Mysterious box was found on the beach. *

*Despite spending probably years in the water, it still works! *

*But what does it do? *

*It has four inputs (electric) & a switch. Setting these inputs and different switch positions emits various weird and scary sounds as output in response to the electric signals. *

*It sizzles, gurgles, hisses, ominously tics like a bomb,etc.....but nothing happens - just sounds. So no harm will happen to surroundings.*

As we can see from the scenario, the task now in this challenge, is to predict the sounds that the *Mysterios Box* will make upon providing various set of inputs and different switch positions.

Henceforth, we will refer to this *mysterious* box as *Black Box*.

Also, since there are only finite number of sounds the box can make, the output *sounds* attribute is a categorical value, which will be predicted in this task.

Students were recommended to first find some correlation between data by using free-style analysis, and then proceed to using any ML models. 

To read more about prediction in R, see sections \@ref(classification),\@ref(regression) and \@ref(models)

Lets look at a snippet of the Mysterious Box/ Black Box dataset used for training the models below.
The training describes which sounds has been noted in the laboratory in nearly 20,000 experiments combining different input signals and switch positions.

```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxtrainApril22.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Black Box Dataset(TRAINING) for Prediction Challenge 4',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")

```

We can see that there are multiple attributes like *INPUT1,2,3,4 and Switch* that can be used as predictors, and then there is *Sound* attribute with ideal values for each record of the experiment record which will be used while training and then will be predicted on the testing dataset.

Lets look at the snippet of the black box dataset for testing.

```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxTestApril22-students.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Black Box Dataset(TESTING) for Prediction Challenge 4',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

We can see that the *Sound* attribute is not present in this dataset, since it is the attribute that will be predicted using our analysis of the training dataset.

Also, lets look at the submission file for prediction challenge 4.

```{r,echo=FALSE}
realestate<-read.csv("https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/BlackBoxTestApril22-submission.csv") #web load
temp<-knitr::kable(
  head(realestate, 10), caption = 'Snippet of Submission file for Prediction Challenge 4',
  booktabs = TRUE
)
library(kableExtra)
kableExtra::scroll_box(temp,width = "100%")
```

We can see there are only 2 columns *ID* and *Sound*. *ID* column's entries corresponds/are similar to the *ID* column of the testing data. Thus we need to just fill the *Sound* column with appropriate earning value predicted by our analysis.

Now that we have seen the data, feel free to go to the Kaggle site of this prediction challenge and take the challenge yourself. The link for challenge: [Prediction Challenge 4](https://www.kaggle.com/t/423f51ea45be4efea1ddb12fee969cfe){target="_blank"}

---

### Top Submissions for Challenge 4

1. Nicole Coria     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred41">Nicole's PPT</button>
<div id="pred41" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/nicolepred41.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was the top submission based on accuracy score, with a final score more than 68.7%
  - The approach to solving this challenge was iterative and trail and error based. 
    - First, since the task is to predict categorical data, she decided to use rpart(directly).
    - Then, over iteration, by varying the control parameters of rpart, she tried to find the model with the highest accuracy.
  - Use of cross-validation also helped in finding the best fit model.
  
  
2. Atharva Patil     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred42">Atharva's PPT</button>
<div id="pred42" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/atharvapred41.pdf&embedded=true" width="100%" height="500px"></embed>
</div>

  - This was another top submission based on accuracy score, with final score above 68%
  - The approach to solving the task was very well implemented, using external resources too.
    - Atharva tried to analyze the data first. To so this, he used Prof. Imielinski's online platform called [Boundless Analytics](http://www.foreveranalytics.com){target="_blank"}.
      - This online platform has ability to analyze the data automatically, and create plots which only matter or provide more information about the data.
      - It eliminates the need to perform the data analysis manually.
  - Then, he proceeded by building the model using the rpart() function and control parameters.


3. Andrew Scovell     <button class="btn btn-primary" data-toggle="collapse" data-target="#pred43">Andrew's PPT</button>
<div id="pred43" class="collapse">    
<embed src="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/pred/andrewpred4.pdf&embedded=true" width="100%" height="500px"></embed>
</div>
    
  - Bennett had a final accuracy score of above 68% and was one of the top submissions for this challenge.
  - He did a very extensive data analysis using all the attributes of the dataset.
    - He also tried analyzing using mean, sums, standard deviation, etc of the numerical inputs.
  - Using the control parameters of the rpart() function he tried to find the best fitting model, and used cross-validation to avoid overfitting.
  
---

To perform any of the above challenges yourself, visit the appropriate links.

1. Prediction Challenge 1 [https://www.kaggle.com/t/8099c3c8bd5940928d102a6ddda0ee3d](https://www.kaggle.com/t/8099c3c8bd5940928d102a6ddda0ee3d){target="_blank"}
1. Prediction Challenge 2 [https://www.kaggle.com/t/607a8221c6a647048f88ffa380ad1e4b](https://www.kaggle.com/t/607a8221c6a647048f88ffa380ad1e4b){target="_blank"}
1. Prediction Challenge 3 [https://www.kaggle.com/t/951a9ad1d7e9444bb29b0dca65aed1cd](https://www.kaggle.com/t/951a9ad1d7e9444bb29b0dca65aed1cd){target="_blank"}
1. Prediction Challenge 4 [https://www.kaggle.com/t/423f51ea45be4efea1ddb12fee969cfe](https://www.kaggle.com/t/423f51ea45be4efea1ddb12fee969cfe){target="_blank"}
    

<!--chapter:end:chapters/predblogs.Rmd-->

